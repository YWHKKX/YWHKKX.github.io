<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="实验介绍在本练习中，您将实现神经网络的反向传播算法，并将其应用于手写数字识别任务  ex4.m - Octave&#x2F;MATLAB 脚本帮助您完成练习 ex4data1.mat - 手写数字训练集 ex4weights.mat - 神经网络训练的初始权重 submit.m - 提交脚本，将您的解决方案发送到我们的服务器 displayData.m - 帮助可视化数据集的函数 fmincg.m - 功能">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine-Learning-Lab4">
<meta property="og:url" content="http://example.com/2022/05/15/Machine-Learning-Lab4/index.html">
<meta property="og:site_name" content="Pwn进你的心">
<meta property="og:description" content="实验介绍在本练习中，您将实现神经网络的反向传播算法，并将其应用于手写数字识别任务  ex4.m - Octave&#x2F;MATLAB 脚本帮助您完成练习 ex4data1.mat - 手写数字训练集 ex4weights.mat - 神经网络训练的初始权重 submit.m - 提交脚本，将您的解决方案发送到我们的服务器 displayData.m - 帮助可视化数据集的函数 fmincg.m - 功能">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2022/05/15/Machine-Learning-Lab4/1652529225830.png">
<meta property="og:image" content="http://example.com/2022/05/15/Machine-Learning-Lab4/1652529620297.png">
<meta property="og:image" content="http://example.com/2022/05/15/Machine-Learning-Lab4/1652530998468.png">
<meta property="og:image" content="http://example.com/2022/05/15/Machine-Learning-Lab4/1652602690664.png">
<meta property="article:published_time" content="2022-05-15T08:19:51.000Z">
<meta property="article:modified_time" content="2022-10-09T16:08:18.000Z">
<meta property="article:author" content="yhellow">
<meta property="article:tag" content="labs">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2022/05/15/Machine-Learning-Lab4/1652529225830.png">

<link rel="canonical" href="http://example.com/2022/05/15/Machine-Learning-Lab4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Machine-Learning-Lab4 | Pwn进你的心</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Pwn进你的心</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/15/Machine-Learning-Lab4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yhellow">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Pwn进你的心">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Machine-Learning-Lab4
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-05-15 16:19:51" itemprop="dateCreated datePublished" datetime="2022-05-15T16:19:51+08:00">2022-05-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-10-10 00:08:18" itemprop="dateModified" datetime="2022-10-10T00:08:18+08:00">2022-10-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>14k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>12 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="实验介绍"><a href="#实验介绍" class="headerlink" title="实验介绍"></a>实验介绍</h2><p>在本练习中，您将实现神经网络的反向传播算法，并将其应用于手写数字识别任务</p>
<ul>
<li>ex4.m - Octave/MATLAB 脚本帮助您完成练习</li>
<li>ex4data1.mat - 手写数字训练集</li>
<li>ex4weights.mat - 神经网络训练的初始权重</li>
<li>submit.m - 提交脚本，将您的解决方案发送到我们的服务器</li>
<li>displayData.m - 帮助可视化数据集的函数</li>
<li>fmincg.m - 功能最小化例行程序（类似于fminunc）</li>
<li>sigmoid.m - Sigmoid 函数（假设陈述）</li>
<li>computeNumericalGradient.m - 计算梯度(倒数)的函数</li>
<li>checkNNGradients.m - 帮助检查梯度的函数（梯度检测）</li>
<li>debugInitializeWeights.m - 初始化权重的函数</li>
<li>predict.m - 神经网络预测函数</li>
<li>[?] sigmoidGradient.m - 计算sigmoid函数的梯度</li>
<li>[?] randInitializeWeights.m - 随机初始化权重</li>
<li>[?] nnCostFunction.m - 神经网络代价函数</li>
</ul>
<h2 id="Neural-Networks（神经网络）"><a href="#Neural-Networks（神经网络）" class="headerlink" title="Neural Networks（神经网络）"></a>Neural Networks（神经网络）</h2><p>在上一个练习中，您实现了神经网络的前馈传播，并使用它使用我们提供的权重预测手写数字</p>
<p>在本练习中，您将实现反向传播算法来学习神经网络的参数，提供的脚本 ex4.m 将帮助你逐步完成这个练习</p>
<p>这与您在上一个练习中使用的数据集相同，ex3data1.mat中有5000个培训示例：</p>
<ul>
<li>其中每个训练示例是数字的 20x20 像素灰度图像</li>
<li>每个像素由一个浮点数表示，表示该位置的灰度强度</li>
<li>20×20 的像素网格被“展开”成400维向量，这些训练示例中的每一个都成为我们的数据矩阵X中的一行</li>
<li>这给了我们一个 5000×400 的矩阵X，其中每一行都是手写数字图像的训练示例</li>
<li>训练集的第二部分是 5000 维向量 y，其中包含训练集的标签</li>
<li>为了与 Octave/MATLAB 索引更兼容，在没有零索引的情况下，我们将数字0映射到值10，因此，“0”数字标记为“10”，而数字“1”至“9”按其自然顺序标记为“1”至“9”</li>
</ul>
<h2 id="Visualizing-the-data（可视化数据）"><a href="#Visualizing-the-data（可视化数据）" class="headerlink" title="Visualizing the data（可视化数据）"></a>Visualizing the data（可视化数据）</h2><p>绘制的过程和上一个实验一样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ==================== 1.读取数据，并显示随机样例 ==============================</span></span><br><span class="line">data = scio.loadmat(<span class="string">&#x27;data\ex4data1.mat&#x27;</span>) <span class="comment"># 使用scipy.io中的函数读取mat文件,data的格式是字典</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据关键字,分别获得输入数据和输出的真值</span></span><br><span class="line">X = data[<span class="string">&#x27;X&#x27;</span>]</span><br><span class="line">Y = data[<span class="string">&#x27;y&#x27;</span>].flatten()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机取出其中的100个样本,显示结果</span></span><br><span class="line">m = X.shape[<span class="number">0</span>] <span class="comment"># m:矩阵长度</span></span><br><span class="line">rand_indices = np.random.permutation(<span class="built_in">range</span>(m)) <span class="comment"># 把[0,m-1]的数据随机排序</span></span><br><span class="line">selected = X[rand_indices[<span class="number">1</span>:<span class="number">100</span>],:] <span class="comment"># 排序后取前100个样本</span></span><br><span class="line">display_data(selected) <span class="comment"># 显示手写数字样例(这里不展示了)</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>绘制的图像：</p>
<img src="/2022/05/15/Machine-Learning-Lab4/1652529225830.png" class width="1652529225830"> 
<h2 id="Model-representation（模型表示）"><a href="#Model-representation（模型表示）" class="headerlink" title="Model representation（模型表示）"></a>Model representation（模型表示）</h2><p>我们的神经网络它有三层——输入层、隐藏层和输出层</p>
<img src="/2022/05/15/Machine-Learning-Lab4/1652529620297.png" class width="1652529620297"> 
<ul>
<li>我们的输入是3位数图像的像素值，由于图像的大小为 20×20，这给了我们 400 个输入层单元（不包括总是输出+1的额外偏置单元）</li>
<li>训练数据将由 ex4.m 脚本加载到变量X和y中</li>
<li>我们已经向您提供了一套我们已经培训过的网络参数（θ(1)，θ(2)）这些都存储在 ex4weights.m 中，并将由 ex4.m 加载</li>
</ul>
<h2 id="Feedforward-and-cost-function（正向传播和代价函数）"><a href="#Feedforward-and-cost-function（正向传播和代价函数）" class="headerlink" title="Feedforward and cost function（正向传播和代价函数）"></a>Feedforward and cost function（正向传播和代价函数）</h2><p>现在你将实现神经网络的代价函数和梯度，首先，在 nnCostFunction.m 中完成代码，神经网络的代价函数是：</p>
<script type="math/tex; mode=display">
J(θ)=-\frac{1}{m}[\sum_{i=1}^{m}\sum_{k=1}^{K}y_k^{(i)}log(h_θ(x^{(i)}))_k+(1-y_k^{(i)})log(1-(h_θ(x^{(i)}))_k)]+
\frac{λ}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s1}\sum_{j=1}^{s1+1}(θ_{ji}^{(l)})^2</script><p>这里的 hθ(x) 就可以是逻辑回归中的 Sigmoid 函数（假设陈述），而 θ 就是模型的参数向量（在神经网络中也被称为“权重”）</p>
<p>K=10 是可能标签的总数，注意：虽然原始标签（在变量y中）是 1,2,3……10 为了训练神经网络，我们需要将标签重新编码为只包含值0或1的向量</p>
<img src="/2022/05/15/Machine-Learning-Lab4/1652530998468.png" class width="1652530998468"> 
<p>实现过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ==================== 2.读取参数，并计算代价 ==================================</span></span><br><span class="line">weights = scio.loadmat(<span class="string">&#x27;data\ex4weights.mat&#x27;</span>)</span><br><span class="line">theta1 = weights[<span class="string">&#x27;Theta1&#x27;</span>]	</span><br><span class="line">theta2 = weights[<span class="string">&#x27;Theta2&#x27;</span>]	</span><br><span class="line">nn_paramters = np.concatenate([theta1.flatten(),theta2.flatten()],axis =<span class="number">0</span>) <span class="comment"># 把theta1,theta2转化为一维数组后,进行拼接</span></span><br><span class="line"><span class="comment"># 设置参数</span></span><br><span class="line">input_layer = <span class="number">400</span></span><br><span class="line">hidden_layer = <span class="number">25</span></span><br><span class="line">out_layer = <span class="number">10</span></span><br><span class="line"><span class="comment"># 计算代价,无正则项</span></span><br><span class="line">lmd = <span class="number">0</span></span><br><span class="line">cost,grad = nn_cost_function(X,Y,nn_paramters,input_layer,hidden_layer,out_layer,lmd) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Cost at parameters (loaded from ex4weights): &#123;:0.6f&#125;\n(This value should be about 0.287629)&#x27;</span>.<span class="built_in">format</span>(cost))</span><br><span class="line"><span class="comment"># 计算代价,带入正则项</span></span><br><span class="line">lmd = <span class="number">1</span></span><br><span class="line">cost,grad = nn_cost_function(X,Y,nn_paramters,input_layer,hidden_layer,out_layer,lmd)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Cost at parameters (loaded from ex4weights): &#123;:0.6f&#125;\n(This value should be about 0.383770)&#x27;</span>.<span class="built_in">format</span>(cost))</span><br><span class="line"><span class="comment"># 验证sigmoid的梯度</span></span><br><span class="line">g = sigmoid_gradient(np.array([-<span class="number">1</span>, -<span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Sigmoid gradient evaluated at [-1  -0.5  0  0.5  1]:\n&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(g))</span><br></pre></td></tr></table></figure>
<ul>
<li>flatten()：把数组变成一列的形式，等价于 reshape</li>
<li>concatenate(a1,a2,…)：能够一次完成多个数组的拼接，其中 a1,a2,… 是数组类型的参数 </li>
</ul>
<p>接下来就分析分析最核心的两个函数：sigmoid_gradient，nn_cost_function</p>
<ul>
<li>sigmoid_gradient：计算 sigmoid 函数的梯度（后面会详细分析）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">z</span>):</span></span><br><span class="line">	g = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z)) <span class="comment"># 就是Sigmoid函数</span></span><br><span class="line">	<span class="keyword">return</span> g</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_gradient</span>(<span class="params">z</span>):</span></span><br><span class="line">	grad = sigmoid(z) * (<span class="number">1</span>-sigmoid(z))</span><br><span class="line">	<span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure>
<ul>
<li>nn_cost_function：用于计算代价（代价函数-交叉熵）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sigmoid <span class="keyword">import</span> sigmoid</span><br><span class="line"><span class="keyword">from</span> sigmoid <span class="keyword">import</span> sigmoid_gradient</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_cost_function</span>(<span class="params">X,Y,nn_paramters,input_layer,hidden_layer,out_layer,lmd=<span class="number">0</span></span>):</span></span><br><span class="line">    </span><br><span class="line">	theta1 = nn_paramters[:hidden_layer*(input_layer+<span class="number">1</span>)].reshape(hidden_layer,input_layer+<span class="number">1</span>) <span class="comment"># 取出theta1</span></span><br><span class="line">	theta2 = nn_paramters[hidden_layer*(input_layer+<span class="number">1</span>):].reshape(out_layer,hidden_layer+<span class="number">1</span>) <span class="comment"># 取出theta2</span></span><br><span class="line">	m = Y.size <span class="comment"># 获取样本数目</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 输入层的输出等于输入,X增加一列偏置维度</span></span><br><span class="line">	a1 = np.column_stack((np.ones(X.shape[<span class="number">0</span>]),X)) <span class="comment"># 5000*401</span></span><br><span class="line">	<span class="comment"># 隐藏层的输入和输出</span></span><br><span class="line">	z2 = a1.dot(theta1.T) <span class="comment"># 5000*25</span></span><br><span class="line">	a2 = sigmoid(z2)</span><br><span class="line">	a2 = np.column_stack((np.ones(a2.shape[<span class="number">0</span>]),a2)) <span class="comment"># 5000*26</span></span><br><span class="line">	<span class="comment"># 输出层的输入和输出</span></span><br><span class="line">	z3 = a2.dot(theta2.T) <span class="comment"># 5000*10</span></span><br><span class="line">	a3 = sigmoid(z3) <span class="comment"># 5000*10</span></span><br><span class="line">    </span><br><span class="line">	<span class="comment"># a3[m,k]表示第m个样本预测属于k的概率(因为激活函数是logistic函数)</span></span><br><span class="line">	<span class="comment"># 根据Y的值,也转换成和a3相同格式的数组</span></span><br><span class="line">	<span class="comment"># yk中每一行只能有一列值为1,yk[m,k]=1表示第m个样本的真实输出是k,其他列为0</span></span><br><span class="line">	yk = np.zeros((m,out_layer))</span><br><span class="line">	<span class="comment"># 注意:Y中的取值范围是[1,10],而yk中的列下标范围是[0,9]</span></span><br><span class="line">	<span class="keyword">for</span> num <span class="keyword">in</span> <span class="built_in">range</span>(Y.size):</span><br><span class="line">		yk[num,Y[num]-<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">	<span class="comment"># 计算代价,因为输出层的激活函数是logistic函数,所有代价也是以logistic regression代价函数</span></span><br><span class="line">	cost_arr = - yk * np.log(a3) - (<span class="number">1</span>-yk) * np.log(<span class="number">1</span>-a3)</span><br><span class="line">	cost = cost_arr.<span class="built_in">sum</span>()/m + lmd /(<span class="number">2</span>*m) *( (theta1[:,<span class="number">1</span>:] **<span class="number">2</span>).<span class="built_in">sum</span>() + (theta2[:,<span class="number">1</span>:] **<span class="number">2</span>).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 使用BP算法计算梯度</span></span><br><span class="line">	delta3 = a3 - yk <span class="comment"># 5000*10</span></span><br><span class="line"></span><br><span class="line">	delta2 = delta3.dot(theta2) * sigmoid_gradient(np.column_stack((np.ones(z2.shape[<span class="number">0</span>]),z2)))</span><br><span class="line">	delta2 = delta2[:,<span class="number">1</span>:] <span class="comment"># 5000*25</span></span><br><span class="line">	<span class="comment"># theta1的梯度</span></span><br><span class="line">	theta1_grad = np.zeros(theta1.shape) <span class="comment"># 25 x 401</span></span><br><span class="line">	theta1_grad = theta1_grad + (delta2.T).dot(a1) <span class="comment"># 25*401</span></span><br><span class="line">	nn_parameter1_grad = theta1_grad/m + (lmd/m) * np.column_stack((np.zeros(theta1.shape[<span class="number">0</span>]),theta1[:,<span class="number">1</span>:]))</span><br><span class="line">	<span class="comment"># theta2的梯度</span></span><br><span class="line">	theta2_grad = np.zeros(theta2.shape) <span class="comment"># 10 x 26</span></span><br><span class="line">	theta2_grad = theta2_grad + (delta3.T).dot(a2) <span class="comment"># 10*26</span></span><br><span class="line">	nn_parameter2_grad = theta2_grad/m + (<span class="number">1</span>/m) * np.column_stack((np.zeros(theta2.shape[<span class="number">0</span>]),theta2[:,<span class="number">1</span>:]))</span><br><span class="line">	<span class="comment"># 返回梯度</span></span><br><span class="line">	grad = np.concatenate([nn_parameter1_grad.flatten(),nn_parameter2_grad.flatten()])</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> cost,grad</span><br></pre></td></tr></table></figure>
<ul>
<li>上一个实验是直接导入的 theta1，theta2，而这个实验是用 <strong>交叉熵</strong> 计算出来的</li>
<li>其中包括了反向传播算法（BP算法）来计算梯度，后面会进行分析</li>
<li>PS：高数和线代太菜了，数学上的理解有点困难，所以我就不折磨自己了</li>
</ul>
<h2 id="Backpropagation（反向传播）"><a href="#Backpropagation（反向传播）" class="headerlink" title="Backpropagation（反向传播）"></a>Backpropagation（反向传播）</h2><p>在这部分练习中，您将实现反向传播算法来计算神经网络代价函数的梯度</p>
<ul>
<li>上一阶段完成的 nnCostFunction.m 会返回一个合适的梯度值（本阶段还会分析一下 nnCostFunction.m 中，使用BP算法计算梯度的那部分）</li>
<li>一旦计算出梯度，就可以通过使用先进的优化器 fmincg（类似于fminunc）最小化代价函数 J(θ)，训练神经网络</li>
<li>首先，实现反向传播算法来计算（未规范化）神经网络参数的梯度</li>
<li>在验证了针对非正则化情况的梯度计算是正确的之后，您将实现正则化神经网络的梯度</li>
</ul>
<p><strong>Sigmoid gradient（Sigmoid 梯度）</strong></p>
<p>为了帮助您开始这部分练习，您将首先实现 sigmoid gradient 函数，用于计算 Sigmoid 梯度</p>
<p>sigmoid 函数的梯度计算公式为：</p>
<script type="math/tex; mode=display">
sigmoid(z)=g(z)=\frac{1}{1+e^{-z}}\\
g^{'}(z)=\frac{d}{d_{z}}g(z)=g(z)(1-g(z))</script><p>实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">z</span>):</span></span><br><span class="line">	g = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z)) <span class="comment"># 就是Sigmoid函数</span></span><br><span class="line">	<span class="keyword">return</span> g</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_gradient</span>(<span class="params">z</span>):</span></span><br><span class="line">	grad = sigmoid(z) * (<span class="number">1</span>-sigmoid(z)) <span class="comment"># g(z)=g(z)(1-g(z))</span></span><br><span class="line">	<span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure>
<p><strong>Random initialization（随机初始化）</strong></p>
<p>在训练神经网络时，重要的是随机初始化对称性破坏的参数</p>
<ul>
<li>随机初始化的一个有效策略是在范围内均匀地随机选择 θ(l) 的值（范围是：[−init, init]）</li>
<li>您应该使用 init(ε)=0.12.2 ，这个值范围确保参数保持较小，并使学习更有效</li>
<li>你的工作是完成 randInitializeWeights.m 初始化θ的权重</li>
</ul>
<p>选择 init 的一个有效策略是基于神经网络中的单元数：</p>
<script type="math/tex; mode=display">
ε_{init}=\frac{\sqrt{6}}{\sqrt{L_{in}+L_{out}}}\,\,\,\,
(L_{in}=s_l,L_{out}=s_{l+1})\\
PS:s_l和s_{l+1}是相邻层中的单元数</script><p>实现代码为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化网络参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rand_init_weights</span>(<span class="params">L_in,L_out</span>):</span></span><br><span class="line">	epsilon = np.sqrt(<span class="number">6</span>) / np.sqrt(L_in + L_out)</span><br><span class="line">	init_theta = np.random.random((L_out,L_in+<span class="number">1</span>)) * <span class="number">2</span>*epsilon - epsilon</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> init_theta</span><br></pre></td></tr></table></figure>
<ul>
<li>sqrt(x)：对“x”开平方</li>
<li>random.random(x,y)：获取一个范围在 [x,y] 的随机浮点数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =========================== 3.初始化网络参数 =================================</span></span><br><span class="line">random_theta1 = rand_init_weights(input_layer,hidden_layer) <span class="comment"># 初始化网络参数 </span></span><br><span class="line">random_theta2 = rand_init_weights(hidden_layer,out_layer) <span class="comment"># 初始化网络参数</span></span><br><span class="line">rand_nn_parameters = np.concatenate([random_theta1.flatten(),random_theta2.flatten()]) <span class="comment"># 组合后的随机参数(θ集)</span></span><br><span class="line"></span><br><span class="line">lmd =<span class="number">3</span></span><br><span class="line">check_nn_gradients(lmd) <span class="comment"># 梯度检测</span></span><br><span class="line">debug_cost, _ = nn_cost_function(X,Y,nn_paramters,input_layer,hidden_layer,out_layer,lmd) <span class="comment"># 代价函数,用于计算初始代价值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Cost at (fixed) debugging parameters (w/ lambda = &#123;&#125;): &#123;:0.6f&#125;\n(for lambda = 3, this value should be about 0.576051)&#x27;</span>.<span class="built_in">format</span>(lmd, debug_cost))</span><br></pre></td></tr></table></figure>
<p><strong>Backpropagation（反向传播的核心算法）</strong></p>
<p>对于反向传播，其实就是进行了如下的一次运算：</p>
<script type="math/tex; mode=display">
δ^{(l)}_{j}=a^{(l)}_{j}-y_j</script><ul>
<li>计算出各个层的 “δ(l,j)”，代表了第 l 层的第 j 结点的误差</li>
</ul>
<p>计算案例：</p>
<script type="math/tex; mode=display">
For\,each\,output\,unit(layer\,\,L=4)\\δ^{(4)}=a^{(4)}-y\\δ^{(3)}=(θ^{(3)})^{T}δ^{(4)}.*g^{'}(z^{(3)})\\δ^{(2)}=(θ^{(3)})^{T}δ^{(3)}.*g^{'}(z^{(2)})</script><ul>
<li>对于最后一层（输出层），δ4 就是 a4（输出层预测的结果）和 y（真实的结果）之间的差值</li>
<li>而对于中间的隐藏层，因为不清楚“预测结果”和“真实结果”的具体值，所以就只能通过以上的公式进行模拟计算</li>
</ul>
<p>最后，回顾一下“代价函数-交叉熵”的计算过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sigmoid <span class="keyword">import</span> sigmoid</span><br><span class="line"><span class="keyword">from</span> sigmoid <span class="keyword">import</span> sigmoid_gradient</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_cost_function</span>(<span class="params">X,Y,nn_paramters,input_layer,hidden_layer,out_layer,lmd=<span class="number">0</span></span>):</span></span><br><span class="line">    </span><br><span class="line">	theta1 = nn_paramters[:hidden_layer*(input_layer+<span class="number">1</span>)].reshape(hidden_layer,input_layer+<span class="number">1</span>) <span class="comment"># 取出theta1</span></span><br><span class="line">	theta2 = nn_paramters[hidden_layer*(input_layer+<span class="number">1</span>):].reshape(out_layer,hidden_layer+<span class="number">1</span>) <span class="comment"># 取出theta2</span></span><br><span class="line">	m = Y.size <span class="comment"># 获取样本数目</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 输入层的输出等于输入,X增加一列偏置维度</span></span><br><span class="line">	a1 = np.column_stack((np.ones(X.shape[<span class="number">0</span>]),X)) <span class="comment"># 5000*401</span></span><br><span class="line">	<span class="comment"># 隐藏层的输入和输出</span></span><br><span class="line">	z2 = a1.dot(theta1.T) <span class="comment"># 5000*25</span></span><br><span class="line">	a2 = sigmoid(z2)</span><br><span class="line">	a2 = np.column_stack((np.ones(a2.shape[<span class="number">0</span>]),a2)) <span class="comment"># 5000*26</span></span><br><span class="line">	<span class="comment"># 输出层的输入和输出</span></span><br><span class="line">	z3 = a2.dot(theta2.T) <span class="comment"># 5000*10</span></span><br><span class="line">	a3 = sigmoid(z3) <span class="comment"># 5000*10</span></span><br><span class="line">    </span><br><span class="line">	<span class="comment"># a3[m,k]表示第m个样本预测属于k的概率(因为激活函数是logistic函数)</span></span><br><span class="line">	<span class="comment"># 根据Y的值,也转换成和a3相同格式的数组</span></span><br><span class="line">	<span class="comment"># yk中每一行只能有一列值为1,yk[m,k]=1表示第m个样本的真实输出是k,其他列为0</span></span><br><span class="line">	yk = np.zeros((m,out_layer))</span><br><span class="line">	<span class="comment"># 注意:Y中的取值范围是[1,10],而yk中的列下标范围是[0,9]</span></span><br><span class="line">	<span class="keyword">for</span> num <span class="keyword">in</span> <span class="built_in">range</span>(Y.size):</span><br><span class="line">		yk[num,Y[num]-<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">	<span class="comment"># 计算代价,因为输出层的激活函数是logistic函数,所有代价也是以logistic regression代价函数</span></span><br><span class="line">	cost_arr = - yk * np.log(a3) - (<span class="number">1</span>-yk) * np.log(<span class="number">1</span>-a3)</span><br><span class="line">	cost = cost_arr.<span class="built_in">sum</span>()/m + lmd /(<span class="number">2</span>*m) *( (theta1[:,<span class="number">1</span>:] **<span class="number">2</span>).<span class="built_in">sum</span>() + (theta2[:,<span class="number">1</span>:] **<span class="number">2</span>).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 使用BP算法计算梯度</span></span><br><span class="line">	delta3 = a3 - yk <span class="comment"># 5000*10</span></span><br><span class="line"></span><br><span class="line">	delta2 = delta3.dot(theta2) * sigmoid_gradient(np.column_stack((np.ones(z2.shape[<span class="number">0</span>]),z2)))</span><br><span class="line">	delta2 = delta2[:,<span class="number">1</span>:] <span class="comment"># 5000*25</span></span><br><span class="line">	<span class="comment"># theta1的梯度</span></span><br><span class="line">	theta1_grad = np.zeros(theta1.shape) <span class="comment"># 25 x 401</span></span><br><span class="line">	theta1_grad = theta1_grad + (delta2.T).dot(a1) <span class="comment"># 25*401</span></span><br><span class="line">	nn_parameter1_grad = theta1_grad/m + (lmd/m) * np.column_stack((np.zeros(theta1.shape[<span class="number">0</span>]),theta1[:,<span class="number">1</span>:]))</span><br><span class="line">	<span class="comment"># theta2的梯度</span></span><br><span class="line">	theta2_grad = np.zeros(theta2.shape) <span class="comment"># 10 x 26</span></span><br><span class="line">	theta2_grad = theta2_grad + (delta3.T).dot(a2) <span class="comment"># 10*26</span></span><br><span class="line">	nn_parameter2_grad = theta2_grad/m + (<span class="number">1</span>/m) * np.column_stack((np.zeros(theta2.shape[<span class="number">0</span>]),theta2[:,<span class="number">1</span>:]))</span><br><span class="line">	<span class="comment"># 返回梯度</span></span><br><span class="line">	grad = np.concatenate([nn_parameter1_grad.flatten(),nn_parameter2_grad.flatten()])</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> cost,grad</span><br></pre></td></tr></table></figure>
<h2 id="Model-Training（模型训练）"><a href="#Model-Training（模型训练）" class="headerlink" title="Model Training（模型训练）"></a>Model Training（模型训练）</h2><p>代码实现：这里采用 fmincg 来代替梯度下降</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ========================== 4.训练NN ==========================================</span></span><br><span class="line">lmd = <span class="number">1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_func</span>(<span class="params">p</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn_cost_function(X,Y,p,input_layer,hidden_layer,out_layer,lmd)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_func</span>(<span class="params">p</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn_cost_function(X,Y,p,input_layer,hidden_layer,out_layer,lmd)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">nn_params, *unused = opt.fmin_cg(cost_func, fprime=grad_func, x0=rand_nn_parameters, maxiter=<span class="number">400</span>, disp=<span class="literal">True</span>, full_output=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从返回结果nn_params中获取θ1和θ2(拟合完毕)</span></span><br><span class="line">theta1 = nn_params[:hidden_layer * (input_layer + <span class="number">1</span>)].reshape(hidden_layer, input_layer + <span class="number">1</span>)</span><br><span class="line">theta2 = nn_params[hidden_layer * (input_layer + <span class="number">1</span>):].reshape(out_layer, hidden_layer + <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>fmincg 是一种高效的迭代器，它的需要主要参数依次为：<ul>
<li>nn_cost_function 返回的代价</li>
<li>nn_cost_function 返回的梯度</li>
<li>rand_nn_parameters 中存储的随机参数集</li>
</ul>
</li>
</ul>
<h2 id="Gradient-checking（梯度检测）"><a href="#Gradient-checking（梯度检测）" class="headerlink" title="Gradient checking（梯度检测）"></a>Gradient checking（梯度检测）</h2><p>梯度检测会估计梯度（导数）值，然后和你程序计算出来的梯度的值进行对比，以判断程序算出的梯度值是否正确</p>
<p>公式为：</p>
<script type="math/tex; mode=display">
\frac{J(θ_{1}+ε,θ_{2},θ_{3},...,θ_{n})-J(θ_{1}-ε,θ_{2},θ_{3},...,θ_{n})}{2ε}</script><p>实现过程为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> debugInitializeWeights <span class="keyword">as</span> diw <span class="comment"># 初始化权重的函数</span></span><br><span class="line"><span class="keyword">import</span> costFunction <span class="keyword">as</span> ncf <span class="comment"># 计算代价的函数</span></span><br><span class="line"><span class="keyword">import</span> computeNumericalGradient <span class="keyword">as</span> cng <span class="comment"># 计算梯度(倒数)的函数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_nn_gradients</span>(<span class="params">lmd</span>):</span></span><br><span class="line">    input_layer_size = <span class="number">3</span></span><br><span class="line">    hidden_layer_size = <span class="number">5</span></span><br><span class="line">    num_labels = <span class="number">3</span></span><br><span class="line">    m = <span class="number">5</span></span><br><span class="line">    <span class="comment"># We generatesome &#x27;random&#x27; test data</span></span><br><span class="line">    theta1 = diw.debug_initialize_weights(hidden_layer_size, input_layer_size)</span><br><span class="line">    theta2 = diw.debug_initialize_weights(num_labels, hidden_layer_size)</span><br><span class="line">    <span class="comment"># Reusing debugInitializeWeights to genete X</span></span><br><span class="line">    X = diw.debug_initialize_weights(m, input_layer_size - <span class="number">1</span>)</span><br><span class="line">    y = <span class="number">1</span> + np.mod(np.arange(<span class="number">1</span>, m + <span class="number">1</span>), num_labels)</span><br><span class="line">    <span class="comment"># Unroll parameters</span></span><br><span class="line">    nn_params = np.concatenate([theta1.flatten(), theta2.flatten()]) </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_func</span>(<span class="params">p</span>):</span></span><br><span class="line">        <span class="keyword">return</span> ncf.nn_cost_function(X,y,p, input_layer_size, hidden_layer_size, num_labels, lmd)</span><br><span class="line">    cost, grad = cost_func(nn_params) <span class="comment"># 通过我们的&quot;代价函数cost_func&quot;计算梯度</span></span><br><span class="line">    numgrad = cng.compute_numerial_gradient(cost_func, nn_params) <span class="comment"># 直接计算梯度</span></span><br><span class="line">    <span class="built_in">print</span>(np.c_[grad, numgrad]) <span class="comment"># 打印结果</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化权重的函数  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">debug_initialize_weights</span>(<span class="params">fan_out, fan_in</span>):</span></span><br><span class="line">    w = np.zeros((fan_out, <span class="number">1</span> + fan_in))</span><br><span class="line">    w = np.sin(np.arange(w.size)).reshape(w.shape) / <span class="number">10</span></span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算梯度(倒数)的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_numerial_gradient</span>(<span class="params">cost_func, theta</span>):</span></span><br><span class="line">    numgrad = np.zeros(theta.size)</span><br><span class="line">    perturb = np.zeros(theta.size)</span><br><span class="line">    e = <span class="number">1e-4</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">range</span>(theta.size):</span><br><span class="line">        perturb[p] = e</span><br><span class="line">        loss1, grad1 = cost_func(theta - perturb)</span><br><span class="line">        loss2, grad2 = cost_func(theta + perturb)</span><br><span class="line"></span><br><span class="line">        numgrad[p] = (loss2 - loss1) / (<span class="number">2</span> * e)</span><br><span class="line">        perturb[p] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> numgrad</span><br></pre></td></tr></table></figure>
<ul>
<li>在本实验中，我们只对“lmd=3”进行了梯度检测，检测结果如下：</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[[ <span class="number">0.00901304</span>  <span class="number">0.00901304</span>]</span><br><span class="line"> [ <span class="number">0.05042745</span>  <span class="number">0.05042745</span>]</span><br><span class="line"> [ <span class="number">0.05455088</span>  <span class="number">0.05455088</span>]</span><br><span class="line"> [ <span class="number">0.00852048</span>  <span class="number">0.00852048</span>]</span><br><span class="line"> [ <span class="number">0.01171933</span>  <span class="number">0.01171933</span>]</span><br><span class="line"> [<span class="number">-0.05760601</span> <span class="number">-0.05760601</span>]</span><br><span class="line"> [<span class="number">-0.01659828</span> <span class="number">-0.01659828</span>]</span><br><span class="line"> [ <span class="number">0.03966983</span>  <span class="number">0.03966983</span>]</span><br><span class="line"> [ <span class="number">0.00366088</span>  <span class="number">0.00366088</span>]</span><br><span class="line"> [ <span class="number">0.02471166</span>  <span class="number">0.02471166</span>]</span><br><span class="line"> [<span class="number">-0.03245445</span> <span class="number">-0.03245445</span>]</span><br><span class="line"> [<span class="number">-0.05978209</span> <span class="number">-0.05978209</span>]</span><br><span class="line"> [<span class="number">-0.0077655</span>  <span class="number">-0.0077655</span> ]</span><br><span class="line"> [ <span class="number">0.02526392</span>  <span class="number">0.02526392</span>]</span><br><span class="line"> [ <span class="number">0.05947174</span>  <span class="number">0.05947174</span>]</span><br><span class="line"> [ <span class="number">0.03900152</span>  <span class="number">0.03900152</span>]</span><br><span class="line"> [<span class="number">-0.01206378</span> <span class="number">-0.01206378</span>]</span><br><span class="line"> [<span class="number">-0.05761021</span> <span class="number">-0.05761021</span>]</span><br><span class="line"> [<span class="number">-0.04520795</span> <span class="number">-0.04520795</span>]</span><br><span class="line"> [ <span class="number">0.0087583</span>   <span class="number">0.0087583</span> ]</span><br><span class="line"> [ <span class="number">0.30228635</span>  <span class="number">0.30228635</span>]</span><br><span class="line"> [ <span class="number">0.16784019</span>  <span class="number">0.20149903</span>]</span><br><span class="line"> [ <span class="number">0.16341919</span>  <span class="number">0.19979109</span>]</span><br><span class="line"> [ <span class="number">0.16182059</span>  <span class="number">0.16746539</span>]</span><br><span class="line"> [ <span class="number">0.13164304</span>  <span class="number">0.10137094</span>]</span><br><span class="line"> [ <span class="number">0.12980928</span>  <span class="number">0.09145231</span>]</span><br><span class="line"> [ <span class="number">0.09959317</span>  <span class="number">0.09959317</span>]</span><br><span class="line"> [ <span class="number">0.06275198</span>  <span class="number">0.08903145</span>]</span><br><span class="line"> [ <span class="number">0.06814118</span>  <span class="number">0.10771551</span>]</span><br><span class="line"> [ <span class="number">0.06010838</span>  <span class="number">0.07659312</span>]</span><br><span class="line"> [ <span class="number">0.03765248</span>  <span class="number">0.01589163</span>]</span><br><span class="line"> [ <span class="number">0.02937856</span> <span class="number">-0.01062105</span>]</span><br><span class="line"> [ <span class="number">0.09693242</span>  <span class="number">0.09693242</span>]</span><br><span class="line"> [ <span class="number">0.057304</span>    <span class="number">0.07411068</span>]</span><br><span class="line"> [ <span class="number">0.06636988</span>  <span class="number">0.10599418</span>]</span><br><span class="line"> [ <span class="number">0.06353249</span>  <span class="number">0.089544</span>  ]</span><br><span class="line"> [ <span class="number">0.04192228</span>  <span class="number">0.03040615</span>]</span><br><span class="line"> [ <span class="number">0.02820396</span> <span class="number">-0.01025194</span>]]</span><br></pre></td></tr></table></figure>
<ul>
<li>左边是用我们的代价函数，计算出来的梯度</li>
<li>右边是用数学方法，计算出来的梯度</li>
<li>通过对比两者的差值，我们可以大体判断一下该代价函数的效果如何</li>
</ul>
<h2 id="Visualizing-the-hidden-layer（可视化隐藏层）"><a href="#Visualizing-the-hidden-layer（可视化隐藏层）" class="headerlink" title="Visualizing the hidden layer（可视化隐藏层）"></a>Visualizing the hidden layer（可视化隐藏层）</h2><p>理解神经网络学习内容的一种方法是可视化隐藏单元捕获的表示</p>
<p>非正式地说，给定一个特定的隐藏单元，可视化其计算内容的一种方法是找到一个将使其激活的输入“x”</p>
<p>实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ======================= 5.可视化系数和预测 ===================================</span></span><br><span class="line"></span><br><span class="line">display_data(theta1[:, <span class="number">1</span>:]) <span class="comment"># 和之前实验使用的display_data一样</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">pred = predict_nn(X,theta1, theta2) <span class="comment"># 预测神经网络</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Training set accuracy: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(np.mean(pred == Y)*<span class="number">100</span>)) <span class="comment"># 计算该模型的准确度</span></span><br></pre></td></tr></table></figure>
<ul>
<li>实现的原理简单粗暴，直接把输入层输出的激活值放入 display_data 描述数据</li>
<li>注意：上一层输出的激活值，会被当做下一层的输出的数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_data</span>(<span class="params">x</span>):</span></span><br><span class="line">	(m,n) = x.shape</span><br><span class="line">	<span class="comment"># 设置每个小图例的宽度和高度</span></span><br><span class="line">	width = np.<span class="built_in">round</span>(np.sqrt(n)).astype(<span class="built_in">int</span>)</span><br><span class="line">	height = (n / width).astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 设置图片的行数和列数</span></span><br><span class="line">	rows = np.floor(np.sqrt(m)).astype(<span class="built_in">int</span>)</span><br><span class="line">	cols = np.ceil(m / rows).astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 设置图例之间的间隔</span></span><br><span class="line">	pad = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 初始化图像数据</span></span><br><span class="line">	display_array = -np.ones((pad + rows*(height+pad), pad + cols*(width + pad)))</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 把数据按行和列复制进图像中(10x10的表格)</span></span><br><span class="line">	current_image = <span class="number">0</span></span><br><span class="line">	<span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(rows):</span><br><span class="line">		<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(cols):</span><br><span class="line">			<span class="keyword">if</span> current_image &gt; m:</span><br><span class="line">				<span class="keyword">break</span> </span><br><span class="line">			max_val = np.<span class="built_in">max</span>(np.<span class="built_in">abs</span>(x[current_image,:]))</span><br><span class="line">			display_array[pad + j*(height + pad) + np.arange(height),pad + i*(width + pad) + np.arange(width)[:,np.newaxis]] = x[current_image,:].reshape((height,width)) / max_val</span><br><span class="line">			current_image += <span class="number">1</span></span><br><span class="line">		<span class="keyword">if</span> current_image &gt; m :</span><br><span class="line">			<span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 显示图像</span></span><br><span class="line">	plt.figure()</span><br><span class="line">	<span class="comment"># 设置图像色彩为灰度值，指定图像坐标范围</span></span><br><span class="line">	plt.imshow(display_array,cmap = <span class="string">&#x27;gray&#x27;</span>,extent =[-<span class="number">1</span>,<span class="number">1</span>,-<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">	plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">	plt.title(<span class="string">&#x27;Random Seleted Digits&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>把输入的图像数据X进行重新排列，显示在一个面板 figurePane 中</li>
<li>面板中有多个小 imge 用来显示每一行数据</li>
</ul>
<p>描绘结果如下：</p>
<img src="/2022/05/15/Machine-Learning-Lab4/1652602690664.png" class width="1652602690664"> 

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/labs/" rel="tag"><i class="fa fa-tag"></i> labs</a>
              <a href="/tags/Machine-Learning/" rel="tag"><i class="fa fa-tag"></i> Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/05/14/%E6%A1%8C%E5%AE%A0%E5%BC%80%E5%8F%91%E2%85%A1/" rel="prev" title="不务正业系列：桌宠开发Ⅱ（偶尔更新）">
      <i class="fa fa-chevron-left"></i> 不务正业系列：桌宠开发Ⅱ（偶尔更新）
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/05/17/Machine-Learning-Lab5/" rel="next" title="Machine-Learning-Lab5">
      Machine-Learning-Lab5 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">实验介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Networks%EF%BC%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.</span> <span class="nav-text">Neural Networks（神经网络）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Visualizing-the-data%EF%BC%88%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%EF%BC%89"><span class="nav-number">3.</span> <span class="nav-text">Visualizing the data（可视化数据）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-representation%EF%BC%88%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%A4%BA%EF%BC%89"><span class="nav-number">4.</span> <span class="nav-text">Model representation（模型表示）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Feedforward-and-cost-function%EF%BC%88%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%89"><span class="nav-number">5.</span> <span class="nav-text">Feedforward and cost function（正向传播和代价函数）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Backpropagation%EF%BC%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%89"><span class="nav-number">6.</span> <span class="nav-text">Backpropagation（反向传播）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-Training%EF%BC%88%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%EF%BC%89"><span class="nav-number">7.</span> <span class="nav-text">Model Training（模型训练）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-checking%EF%BC%88%E6%A2%AF%E5%BA%A6%E6%A3%80%E6%B5%8B%EF%BC%89"><span class="nav-number">8.</span> <span class="nav-text">Gradient checking（梯度检测）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Visualizing-the-hidden-layer%EF%BC%88%E5%8F%AF%E8%A7%86%E5%8C%96%E9%9A%90%E8%97%8F%E5%B1%82%EF%BC%89"><span class="nav-number">9.</span> <span class="nav-text">Visualizing the hidden layer（可视化隐藏层）</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="yhellow"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">yhellow</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">330</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">170</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yhellow</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">4.9m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">74:38</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
