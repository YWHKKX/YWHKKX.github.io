<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="实验介绍在本练习中，您将实现正则化线性回归，并使用它来研究具有不同偏差-方差特性的模型  ex5.m - Octave&#x2F;MATLAB脚本，帮助您完成练习  ex5data1.mat - 数据集  submit.m - 将您的解决方案发送到我们的服务器  featureNormalize.m - 标准化函数  fmincg.m - 拟合函数，最小化例程（类似于fminunc）  plotFit.m">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine-Learning-Lab5">
<meta property="og:url" content="http://example.com/2022/05/17/Machine-Learning-Lab5/index.html">
<meta property="og:site_name" content="Pwn进你的心">
<meta property="og:description" content="实验介绍在本练习中，您将实现正则化线性回归，并使用它来研究具有不同偏差-方差特性的模型  ex5.m - Octave&#x2F;MATLAB脚本，帮助您完成练习  ex5data1.mat - 数据集  submit.m - 将您的解决方案发送到我们的服务器  featureNormalize.m - 标准化函数  fmincg.m - 拟合函数，最小化例程（类似于fminunc）  plotFit.m">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2022/05/17/Machine-Learning-Lab5/1652700354337.png">
<meta property="og:image" content="http://example.com/2022/05/17/Machine-Learning-Lab5/1652705076474.png">
<meta property="og:image" content="http://example.com/2022/05/17/Machine-Learning-Lab5/1652713389053.png">
<meta property="og:image" content="http://example.com/2022/05/17/Machine-Learning-Lab5/1652768194001.png">
<meta property="og:image" content="http://example.com/2022/05/17/Machine-Learning-Lab5/1652768217539.png">
<meta property="og:image" content="http://example.com/2022/05/17/Machine-Learning-Lab5/1652769307897.png">
<meta property="og:image" content="http://example.com/2022/05/17/Machine-Learning-Lab5/1652769200655.png">
<meta property="og:image" content="http://example.com/2022/05/17/Machine-Learning-Lab5/1652769491692.png">
<meta property="og:image" content="http://example.com/2022/05/17/Machine-Learning-Lab5/1652769505258.png">
<meta property="og:image" content="http://example.com/2022/05/17/Machine-Learning-Lab5/1652770124267.png">
<meta property="og:image" content="http://example.com/2022/05/17/Machine-Learning-Lab5/1652770295732.png">
<meta property="article:published_time" content="2022-05-17T06:52:39.000Z">
<meta property="article:modified_time" content="2022-10-09T16:08:11.389Z">
<meta property="article:author" content="yhellow">
<meta property="article:tag" content="labs">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2022/05/17/Machine-Learning-Lab5/1652700354337.png">

<link rel="canonical" href="http://example.com/2022/05/17/Machine-Learning-Lab5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Machine-Learning-Lab5 | Pwn进你的心</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Pwn进你的心</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/17/Machine-Learning-Lab5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yhellow">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Pwn进你的心">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Machine-Learning-Lab5
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-05-17 14:52:39" itemprop="dateCreated datePublished" datetime="2022-05-17T14:52:39+08:00">2022-05-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-10-10 00:08:11" itemprop="dateModified" datetime="2022-10-10T00:08:11+08:00">2022-10-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>9.5k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>9 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="实验介绍"><a href="#实验介绍" class="headerlink" title="实验介绍"></a>实验介绍</h2><p>在本练习中，您将实现正则化线性回归，并使用它来研究具有不同偏差-方差特性的模型</p>
<ul>
<li>ex5.m - Octave/MATLAB脚本，帮助您完成练习 </li>
<li>ex5data1.mat - 数据集 </li>
<li>submit.m - 将您的解决方案发送到我们的服务器 </li>
<li>featureNormalize.m - 标准化函数 </li>
<li>fmincg.m - 拟合函数，最小化例程（类似于fminunc） </li>
<li>plotFit.m - 绘制多项式拟合图 </li>
<li>trainLinearReg.m - 使用 fmincg 训练线性回归（代价函数为：均方误差）</li>
<li>[?] linearRegCostFunction.m - 正则线性回归代价函数</li>
<li>[?] learningCurve.m - 生成学习曲线 </li>
<li>[?] polyFeatures.m - 将数据映射到多项式特征空间 </li>
<li>[?] validationCurve.m - 生成交叉验证曲线 </li>
</ul>
<h2 id="Regularized-Linear-Regression（正则线性回归）"><a href="#Regularized-Linear-Regression（正则线性回归）" class="headerlink" title="Regularized Linear Regression（正则线性回归）"></a>Regularized Linear Regression（正则线性回归）</h2><p>在本练习的前半部分，您将使用正则化线性回归，利用水库水位的变化来预测流出大坝的水量，在下半部分中，您将完成一些调试学习算法的诊断，并检查偏差与方差的影响 </p>
<h2 id="Visualizing-the-dataset（可视化数据集）"><a href="#Visualizing-the-dataset（可视化数据集）" class="headerlink" title="Visualizing the dataset（可视化数据集）"></a>Visualizing the dataset（可视化数据集）</h2><p>首先，我们将可视化数据集，其中包含：</p>
<ul>
<li>水位变化的历史记录 x</li>
<li>流出大坝的水量 y </li>
</ul>
<p>该数据集分为三个部分：</p>
<ul>
<li>你的模型将学习的训练集：X，y</li>
<li>用于确定正则化参数的交叉验证集：Xval，yval</li>
<li>用于评估性能的测试集，这些是您的模型在培训期间没有看到的“看不见的”示例：Xtest、ytest</li>
</ul>
<p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ============================== 1.读取并显示数据 ==============================</span></span><br><span class="line">data = scio.loadmat(<span class="string">&#x27;data\ex5data1.mat&#x27;</span>)</span><br><span class="line"><span class="comment"># 用于训练模型</span></span><br><span class="line">X = data[<span class="string">&#x27;X&#x27;</span>]</span><br><span class="line">Y = data[<span class="string">&#x27;y&#x27;</span>].flatten()</span><br><span class="line"><span class="comment"># 用于确定正则化参数的交叉验证</span></span><br><span class="line">Xval = data[<span class="string">&#x27;Xval&#x27;</span>]</span><br><span class="line">Yval = data[<span class="string">&#x27;yval&#x27;</span>].flatten()</span><br><span class="line"><span class="comment"># 用于评估性能</span></span><br><span class="line">Xtest = data[<span class="string">&#x27;Xtest&#x27;</span>]</span><br><span class="line">Ytest = data[<span class="string">&#x27;ytest&#x27;</span>].flatten()</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.scatter(X,Y,c=<span class="string">&#x27;r&#x27;</span>,marker=<span class="string">&#x27;x&#x27;</span>) <span class="comment"># 只显示&quot;用于训练模型&quot;的数据</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Change in water level (x)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Water folowing out of the dam (y)&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img src="/2022/05/17/Machine-Learning-Lab5/1652700354337.png" class width="1652700354337"> 
<h2 id="Regularized-linear-regression-cost-function（正则线性回归代价函数）"><a href="#Regularized-linear-regression-cost-function（正则线性回归代价函数）" class="headerlink" title="Regularized linear regression cost function（正则线性回归代价函数）"></a>Regularized linear regression cost function（正则线性回归代价函数）</h2><p>先看下正则化均方误差的公式：</p>
<script type="math/tex; mode=display">
J(θ)=\frac1{2m}[\sum_{i=1}^m(h_θ(x^{(i)})−y^{(i)})^2+λ\sum_{j=1}^nθ_j^{2}]</script><ul>
<li>其中 λ 是控制正则化程度的正则化参数（因此，有助于防止过度拟合）</li>
<li>正则化项对总成本 J(θ) 施加惩罚，随着模型参数 θ 的大小增加，惩罚也增加</li>
<li>注意：不应该正则化θ0项（在 Octave/MATLAB 中，θ0 项表示为 θ(1) ，因为 Octave/MATLAB 中的索引从1开始）</li>
</ul>
<p>相应地，正则化线性回归的代价对 θj 的偏导数定义为：</p>
<script type="math/tex; mode=display">
\frac{∂J(θ)}{∂θ_0}=\frac{1}{m}\sum_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_{j}^{(i)}\quad for\,\,j=0\\
\frac{∂J(θ)}{∂θ_0}=(\frac{1}{m}\sum_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_{j}^{(i)})+\frac{λ}{m}θ_j\quad for\,\,j>0</script><ul>
<li>注意：导数和梯度是一个概念，求解偏导数，就是求解梯度</li>
</ul>
<p>现在，您应该完成文件 linearRegCostFunction.m 中的代码，您的任务是编写一个函数来计算正则化线性回归成本函数，如果可能，尝试将代码矢量化，避免编写循环，然后在本函数中添加代码来计算梯度，并返回变量 grad</p>
<p>实现 linearRegCostFunction 函数：正则线性回归代价函数（均方误差）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算线性回归的代价和梯度&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_cost_function</span>(<span class="params">X,Y,theta,lmd</span>):</span></span><br><span class="line">	m = X.shape[<span class="number">0</span>]</span><br><span class="line">	hyp = X.dot(theta) - Y</span><br><span class="line">	grad = np.zeros(theta.shape) </span><br><span class="line"></span><br><span class="line">	cost = ((hyp.T).dot(hyp) + lmd * (theta.T).dot(theta))/(<span class="number">2</span>*m)</span><br><span class="line"></span><br><span class="line">	temp = (X.T).dot(hyp)</span><br><span class="line">	grad[<span class="number">0</span>] = temp[<span class="number">0</span>]/ m</span><br><span class="line">	grad[<span class="number">1</span>:] = (temp[<span class="number">1</span>:] + lmd * theta[<span class="number">1</span>:])/m</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> cost,grad</span><br></pre></td></tr></table></figure>
<ul>
<li>就是实现了一下上述公式，和实验二的 cost_Function_Reg 一样</li>
</ul>
<p>具体过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ============================ 2.计算代价和梯度 ==============================</span></span><br><span class="line">(m,n)= X.shape</span><br><span class="line">theta = np.ones((n+<span class="number">1</span>))</span><br><span class="line">lmd=<span class="number">1</span></span><br><span class="line">cost,grad = linear_cost_function(np.column_stack((np.ones(m),X)),Y,theta,lmd)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Cost at theta = [1  1]: &#123;:0.6f&#125;\n(this value should be about 303.993192)&#x27;</span>.<span class="built_in">format</span>(cost))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Gradient at theta = [1  1]: &#123;&#125;\n(this value should be about [-15.303016  598.250744]&#x27;</span>.<span class="built_in">format</span>(grad))</span><br></pre></td></tr></table></figure>
<h2 id="Fitting-linear-regression（拟合线性回归）"><a href="#Fitting-linear-regression（拟合线性回归）" class="headerlink" title="Fitting linear regression（拟合线性回归）"></a>Fitting linear regression（拟合线性回归）</h2><p>将在 trainLinearReg.m 中运行代码，来计算 θ 的最佳值（使用 fmincg 拟合代价函数）</p>
<ul>
<li>在这一部分中，我们将正则化参数λ设置为“0”（因为我们目前线性回归的实现是试图拟合二维 θ，所以正则化对如此低维的θ没有明显的帮助）</li>
<li>在本练习的后面部分，您将使用带正则化的多项式回归</li>
<li>最后是 ex5.m 脚本还应绘制最佳拟合线，最佳拟合线会告诉我们：由于数据具有非线性模式，因此模型与数据的拟合度不高</li>
<li>虽然可视化显示最佳拟合是调试学习算法的一种可能方法，但可视化数据和模型并不总是容易的，在下一节中，您将实现一个生成学习曲线的函数，该函数可以帮助您调试学习算法，即使数据不容易可视化</li>
</ul>
<p>实现 trainLinearReg 函数：使用 fmincg 训练线性回归（代价函数为：均方误差）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> linearCostFunction <span class="keyword">import</span> linear_cost_function</span><br><span class="line"><span class="keyword">import</span> scipy.optimize <span class="keyword">as</span> opt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_linear_reg</span>(<span class="params">X,Y,lmd</span>):</span></span><br><span class="line">	init_theta = np.ones(X.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">cost_func</span>(<span class="params">t</span>):</span></span><br><span class="line">		<span class="keyword">return</span> linear_cost_function(X,Y,t,lmd)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">grad_func</span>(<span class="params">t</span>):</span></span><br><span class="line">		<span class="keyword">return</span> linear_cost_function(X,Y,t,lmd)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">	theta,*unused = opt.fmin_cg(cost_func,init_theta,grad_func,maxiter=<span class="number">200</span>,disp=<span class="literal">False</span>,full_output =<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>
<ul>
<li>使用 fmincg 进行拟合</li>
</ul>
<p>具体过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =========================== 3.训练线性回归 ===========================</span></span><br><span class="line">lmd = <span class="number">0</span></span><br><span class="line">theta = train_linear_reg(np.column_stack((np.ones(m),X)),Y,lmd)</span><br><span class="line">plt.plot(X,np.column_stack((np.ones(m),X)).dot(theta))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>绘图结果：</p>
<img src="/2022/05/17/Machine-Learning-Lab5/1652705076474.png" class width="1652705076474"> 
<h2 id="Bias-variance（偏差方差）"><a href="#Bias-variance（偏差方差）" class="headerlink" title="Bias-variance（偏差方差）"></a>Bias-variance（偏差方差）</h2><p>机器学习中的一个重要概念是：偏差方差</p>
<ul>
<li>具有高偏差的模型对于数据来说不够复杂，并且倾向于欠拟合</li>
<li>而具有高方差的模型对训练数据过度拟合</li>
</ul>
<p>在这部分练习中，您将在学习曲线上绘制训练和测试错误，以诊断 “偏差-方差” 问题</p>
<h2 id="Learning-curves-A（学习曲线）"><a href="#Learning-curves-A（学习曲线）" class="headerlink" title="Learning curves A（学习曲线）"></a>Learning curves A（学习曲线）</h2><p>现在，您将实现生成学习曲线的代码，这些曲线在调试学习算法时非常有用</p>
<ul>
<li>为了绘制学习曲线，我们需要使用不同的训练集大小进行训练，得出交叉验证的误差（分别得出训练集，测试集的误差）</li>
<li>要获得不同的训练集大小，应使用原始训练集X的不同子集，可以使用 trainLinearReg 函数来查找θ参数</li>
<li>请注意，lambda 作为参数传递给 learningCurve 函数，在学习θ参数之后，应该计算训练集和交叉验证集的误差</li>
</ul>
<p>实现 learningCurve 函数：生成学习曲线</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> linearCostFunction <span class="keyword">import</span> linear_cost_function <span class="comment"># 正则线性回归代价函数(均方误差)</span></span><br><span class="line"><span class="keyword">from</span> trainLinearRegression <span class="keyword">import</span> train_linear_reg <span class="comment"># 拟合函数fmincg</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learning_curve</span>(<span class="params">X,Y,Xval,Yval,lmd</span>):</span></span><br><span class="line">	m = X.shape[<span class="number">0</span>]</span><br><span class="line">	error_train = np.zeros(m)</span><br><span class="line">	error_val = np.zeros(m)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> num <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">		theta = train_linear_reg(X[<span class="number">0</span>:num+<span class="number">1</span>,:],Y[<span class="number">0</span>:num+<span class="number">1</span>],lmd) <span class="comment"># 使用fmincg训练模型</span></span><br><span class="line"></span><br><span class="line">		error_train[num],_ = linear_cost_function(X[<span class="number">0</span>:num+<span class="number">1</span>,:],Y[<span class="number">0</span>:num+<span class="number">1</span>],theta,lmd) <span class="comment"># 获取训练集的cost代价</span></span><br><span class="line">		error_val[num],_ = linear_cost_function(Xval,Yval,theta,lmd) <span class="comment"># 获取测试集的cost代价</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> error_train,error_val</span><br></pre></td></tr></table></figure>
<ul>
<li>zeros()：返回来一个给定形状和类型的，用“0”填充的数组</li>
</ul>
<p>具体过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =========================== 4.线性回归的学习曲线 ==============</span></span><br><span class="line">lmd = <span class="number">0</span> <span class="comment"># 不包括正则化项</span></span><br><span class="line">error_train,error_val = learning_curve(np.column_stack((np.ones(m),X)),Y,</span><br><span class="line">						np.column_stack((np.ones(Yval.size),Xval)),Yval,lmd)</span><br><span class="line">plt.figure(<span class="number">2</span>)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(m),error_train,<span class="built_in">range</span>(m),error_val)</span><br><span class="line">plt.title(<span class="string">&#x27;Learning Curve for Linear Regression&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;Train&#x27;</span>, <span class="string">&#x27;Cross Validation&#x27;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of Training Examples&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Error&#x27;</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">13</span>, <span class="number">0</span>, <span class="number">150</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<ul>
<li>注意：这里直接用“代价”来表示“误差”，其实它们两个本来就是同一个概念（反正它们都是用同一个公式计算出来的）</li>
</ul>
<img src="/2022/05/17/Machine-Learning-Lab5/1652713389053.png" class width="1652713389053"> 
<ul>
<li>随着训练集数目m的增大，训练集和测试集的误差都逐渐趋于平缓，证明模型欠拟合</li>
</ul>
<h2 id="Polynomial-regression（多项式回归）"><a href="#Polynomial-regression（多项式回归）" class="headerlink" title="Polynomial regression（多项式回归）"></a>Polynomial regression（多项式回归）</h2><p>我们的线性模型的问题是，它对数据来说太简单，导致拟合不足（高偏差）</p>
<p>在本练习的这一部分中，您将通过添加更多功能来解决此问题，对于多项式回归，我们的假设有以下形式： </p>
<script type="math/tex; mode=display">
hθ(x)=θ_0+θ_1(waterLevel)+θ_2(waterLevel)^2+ ... +θ_p(waterLevel)^p\\</script><script type="math/tex; mode=display">
hθ(x)=θ_0+θ_1x_1+θ_2x_2+ ... +θ_px_p</script><p>现在，您将使用“更高的x次方”这一方式（第一个公式），在数据集中添加更多功能，这一部分的任务是完成 polyFeatures 中的代码</p>
<p>实现 polyFeatures 函数：将数据映射到多项式特征空间 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ploy_feature</span>(<span class="params">X,p</span>):</span></span><br><span class="line">	m = X.shape[<span class="number">0</span>] <span class="comment"># 读取矩阵的长度,(&quot;shape[0]&quot;就是读取矩阵第一维度的长度)</span></span><br><span class="line">	X_poly = np.zeros((m,p)) <span class="comment"># 添加更多特征</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> num <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,p+<span class="number">1</span>):</span><br><span class="line">		X_poly[:,num-<span class="number">1</span>] = X.flatten() ** num <span class="comment"># 添加&quot;次方项&quot;</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> X_poly</span><br></pre></td></tr></table></figure>
<ul>
<li>先对矩阵 X 进行“扩充”，然后提高对应特征的次方（姑且这么理解）</li>
</ul>
<p>实现 featureNormalize 函数：把数据特征标准化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_nomalize</span>(<span class="params">X</span>):</span></span><br><span class="line">	mu = np.mean(X,<span class="number">0</span>) <span class="comment"># 计算每一维度的均值</span></span><br><span class="line">	sigma = np.std(X,<span class="number">0</span>,ddof=<span class="number">1</span>) <span class="comment"># 计算沿指定轴的标准差</span></span><br><span class="line">	X_norm = (X - mu)/sigma</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> X_norm,mu,sigma</span><br></pre></td></tr></table></figure>
<ul>
<li>从数据集中减去每个特征的平均值</li>
<li>减去平均值后，再将特征值按各自的“标准偏差”进行缩放（除）</li>
</ul>
<p>具体过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =============================== 5.投影特征为多项式 ================</span></span><br><span class="line">p = <span class="number">8</span></span><br><span class="line"><span class="comment"># 投影和标准化训练集</span></span><br><span class="line">X_poly = ploy_feature(X,p)</span><br><span class="line">X_poly,mu,sigma = feature_nomalize(X_poly)</span><br><span class="line">X_poly = np.column_stack((np.ones(Y.size),X_poly)) <span class="comment"># 将一维数组作为列堆叠到二维数组中</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 投影和标准化验证集</span></span><br><span class="line">X_poly_val = ploy_feature(Xval,p)</span><br><span class="line">X_poly_val -= mu</span><br><span class="line">X_poly_val /= sigma</span><br><span class="line">X_poly_val = np.column_stack((np.ones(Yval.size),X_poly_val))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 投影和标准化测试集</span></span><br><span class="line">X_poly_test = ploy_feature(Xtest,p)</span><br><span class="line">X_poly_test -= mu</span><br><span class="line">X_poly_test /= sigma</span><br><span class="line">X_poly_test = np.column_stack((np.ones(Ytest.size),X_poly_test))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Normalized Training Example 1 : \n&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(X_poly[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
<h2 id="Learning-curves-B（学习曲线）"><a href="#Learning-curves-B（学习曲线）" class="headerlink" title="Learning curves B（学习曲线）"></a>Learning curves B（学习曲线）</h2><p>然后我们利用标准化的多项式数据来绘制学习曲线：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ======================== 6.多项式特征的学习曲线 ===============</span></span><br><span class="line">lmd = <span class="number">0</span></span><br><span class="line"><span class="comment"># 绘制拟合曲线</span></span><br><span class="line">theta = train_linear_reg(X_poly,Y,lmd) <span class="comment"># 拟合函数fmincg </span></span><br><span class="line">x_fit,y_fit = plot_fit(np.<span class="built_in">min</span>(X),np.<span class="built_in">max</span>(X),mu,sigma,theta,p) <span class="comment"># 绘制多项式拟合图</span></span><br><span class="line">plt.figure(<span class="number">3</span>)</span><br><span class="line">plt.scatter(X,Y,c=<span class="string">&#x27;r&#x27;</span>,marker=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.plot(x_fit,y_fit)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Change in water level (x)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Water folowing out of the dam (y)&#x27;</span>)</span><br><span class="line">plt.ylim([-<span class="number">60</span>, <span class="number">40</span>])</span><br><span class="line">plt.title(<span class="string">&#x27;Polynomial Regression Fit (lambda = &#123;&#125;)&#x27;</span>.<span class="built_in">format</span>(lmd))</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 计算代价误差</span></span><br><span class="line">error_train, error_val = learning_curve(X_poly, Y, X_poly_val, Yval, lmd)</span><br><span class="line">plt.figure(<span class="number">4</span>)</span><br><span class="line">plt.plot(np.arange(m), error_train, np.arange(m), error_val)</span><br><span class="line">plt.title(<span class="string">&#x27;Polynomial Regression Learning Curve (lambda = &#123;&#125;)&#x27;</span>.<span class="built_in">format</span>(lmd))</span><br><span class="line">plt.legend([<span class="string">&#x27;Train&#x27;</span>, <span class="string">&#x27;Cross Validation&#x27;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of Training Examples&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Error&#x27;</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">13</span>, <span class="number">0</span>, <span class="number">150</span>])</span><br><span class="line">plt.show()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Polynomial Regression (lambda = &#123;&#125;)&#x27;</span>.<span class="built_in">format</span>(lmd))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;# Training Examples\tTrain Error\t\tCross Validation Error&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;  \t&#123;&#125;\t\t&#123;&#125;\t&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i, error_train[i], error_val[i]))</span><br></pre></td></tr></table></figure>
<p>得到两张图片：（lmd = 0，无正则化）</p>
<p>一，拟合曲线（横坐标：水库水位的变化，纵坐标：从大坝流出的水）：</p>
<img src="/2022/05/17/Machine-Learning-Lab5/1652768194001.png" class width="1652768194001"> 
<ul>
<li>您应该看到多项式拟合能够很好地遵循数据点，因此获得了较低的训练误差</li>
<li>然而，多项式拟合非常复杂，甚至在极端情况下会下降</li>
<li>这是多项式回归模型 <strong>过度拟合</strong> 训练数据并且不能很好地泛化的指标</li>
</ul>
<p>二，学习曲线：</p>
<img src="/2022/05/17/Machine-Learning-Lab5/1652768217539.png" class width="1652768217539"> 
<ul>
<li>注意：蓝线一直在最下面</li>
<li>您可以看到学习曲线在低训练误差低但交叉验证误差高的情况下表现出相同的效果</li>
<li>训练和交叉验证错误之间存在差距，表明存在高方差问题</li>
</ul>
<h2 id="Adjusting-the-regularization-parameter（调整正则化参数）"><a href="#Adjusting-the-regularization-parameter（调整正则化参数）" class="headerlink" title="Adjusting the regularization parameter（调整正则化参数）"></a>Adjusting the regularization parameter（调整正则化参数）</h2><p>在本节中，您将观察正则化参数如何影响正则化多项式回归的偏差方差（主要是通过正则化来消除过拟合的影响）</p>
<p>您现在应该修改 ex5.m 中的 lambda 参数并尝试 λ = [1, 100]，对于这些值中的每一个，脚本应该生成适合数据的多项式以及学习曲线</p>
<p>其实就是把上一部分的 “λ=0” 修改为其他值</p>
<ul>
<li>lmd = 1：</li>
</ul>
<img src="/2022/05/17/Machine-Learning-Lab5/1652769307897.png" class width="1652769307897"> 
<img src="/2022/05/17/Machine-Learning-Lab5/1652769200655.png" class width="1652769200655"> 
<ul>
<li>lmd = 100：</li>
</ul>
<img src="/2022/05/17/Machine-Learning-Lab5/1652769491692.png" class width="1652769491692"> 
<img src="/2022/05/17/Machine-Learning-Lab5/1652769505258.png" class width="1652769505258"> 
<p>可以对比一下“λ=0”，“λ=1”，“λ=100”，对模型过拟合的影响（明显“λ=1”的模型效果最好）</p>
<h2 id="Selecting-λ-using-a-cross-validation-set（使用交叉验证集选择λ）"><a href="#Selecting-λ-using-a-cross-validation-set（使用交叉验证集选择λ）" class="headerlink" title="Selecting λ using a cross validation set（使用交叉验证集选择λ）"></a>Selecting λ using a cross validation set（使用交叉验证集选择λ）</h2><p>从练习的前面部分中，您观察到 λ 的值会显着影响正则化多项式回归，在训练集和交叉验证集上的结果</p>
<ul>
<li>特别是，没有正则化（λ = 0）的模型很好地拟合了训练集，但不能泛化</li>
<li>相反，正则化过多（λ = 100）的模型不能很好地拟合训练集和测试集</li>
<li>一个好的 λ 选择（λ = 1）可以很好地拟合数据</li>
</ul>
<p>在本节中，您将实现一个自动方法来选择 λ 参数，具体来说，您将使用交叉验证集来评估每个 λ 值的好坏，在使用交叉验证集选择最佳 λ 值后，我们可以在测试集上评估模型，以估计模型在实际看不见的数据上的表现</p>
<p>您的任务是完成 validationCurve.m 中的代码</p>
<ul>
<li>具体来说，您应该使用 trainLinearReg 函数使用不同的 λ 值训练模型</li>
<li>并计算训练误差和交叉验证误差</li>
<li>您应该在以下范围内尝试 λ：{0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10}</li>
</ul>
<p>实现 validationCurve 函数：生成交叉验证曲线 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> linearCostFunction <span class="keyword">import</span> linear_cost_function</span><br><span class="line"><span class="keyword">from</span> trainLinearRegression <span class="keyword">import</span> train_linear_reg</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validation_curve</span>(<span class="params">X,Y,Xval,Yval</span>):</span></span><br><span class="line">	lambda_vec = np.array([<span class="number">0.</span>, <span class="number">0.001</span>, <span class="number">0.003</span>, <span class="number">0.01</span>, <span class="number">0.03</span>, <span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>])</span><br><span class="line">	error_train = np.zeros(lambda_vec.size)</span><br><span class="line">	error_val = np.zeros(lambda_vec.size)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> num <span class="keyword">in</span> <span class="built_in">range</span>(lambda_vec.size):</span><br><span class="line">		lmd = lambda_vec[num]</span><br><span class="line">		theta = train_linear_reg(X,Y,lmd)</span><br><span class="line">		error_train[num],_ = linear_cost_function(X,Y,theta,lmd)</span><br><span class="line">		error_val[num],_ = linear_cost_function(Xval,Yval,theta,lmd)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> lambda_vec,error_train,error_val</span><br></pre></td></tr></table></figure>
<p>具体过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ============== 7.通过交叉验证集选择正则项系数lambda =========</span></span><br><span class="line">lambda_vec,error_train,error_val = validation_curve(X_poly,Y,X_poly_test,Ytest)</span><br><span class="line">plt.figure(<span class="number">5</span>)</span><br><span class="line">plt.plot(lambda_vec, error_train, lambda_vec, error_val)</span><br><span class="line">plt.legend([<span class="string">&#x27;Train&#x27;</span>, <span class="string">&#x27;Test Validation&#x27;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;lambda&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Error&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>绘制图像：</p>
<img src="/2022/05/17/Machine-Learning-Lab5/1652770124267.png" class width="1652770124267"> 
<ul>
<li>我们可以看到 λ 的最佳值在“3”左右（由于数据集的训练和验证拆分的随机性，交叉验证误差有时可能低于训练错误）</li>
</ul>
<p>PS：lmd = 3：</p>
<img src="/2022/05/17/Machine-Learning-Lab5/1652770295732.png" class width="1652770295732"> 
<p>可以发现：误差的确比 “lmd=1” 还要小</p>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/labs/" rel="tag"><i class="fa fa-tag"></i> labs</a>
              <a href="/tags/Machine-Learning/" rel="tag"><i class="fa fa-tag"></i> Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/05/15/Machine-Learning-Lab4/" rel="prev" title="Machine-Learning-Lab4">
      <i class="fa fa-chevron-left"></i> Machine-Learning-Lab4
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/05/17/kernel%20UAF/" rel="next" title="kernel UAF">
      kernel UAF <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">实验介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Regularized-Linear-Regression%EF%BC%88%E6%AD%A3%E5%88%99%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%89"><span class="nav-number">2.</span> <span class="nav-text">Regularized Linear Regression（正则线性回归）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Visualizing-the-dataset%EF%BC%88%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%89"><span class="nav-number">3.</span> <span class="nav-text">Visualizing the dataset（可视化数据集）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Regularized-linear-regression-cost-function%EF%BC%88%E6%AD%A3%E5%88%99%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%89"><span class="nav-number">4.</span> <span class="nav-text">Regularized linear regression cost function（正则线性回归代价函数）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fitting-linear-regression%EF%BC%88%E6%8B%9F%E5%90%88%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%89"><span class="nav-number">5.</span> <span class="nav-text">Fitting linear regression（拟合线性回归）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bias-variance%EF%BC%88%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE%EF%BC%89"><span class="nav-number">6.</span> <span class="nav-text">Bias-variance（偏差方差）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Learning-curves-A%EF%BC%88%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF%EF%BC%89"><span class="nav-number">7.</span> <span class="nav-text">Learning curves A（学习曲线）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Polynomial-regression%EF%BC%88%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92%EF%BC%89"><span class="nav-number">8.</span> <span class="nav-text">Polynomial regression（多项式回归）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Learning-curves-B%EF%BC%88%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF%EF%BC%89"><span class="nav-number">9.</span> <span class="nav-text">Learning curves B（学习曲线）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adjusting-the-regularization-parameter%EF%BC%88%E8%B0%83%E6%95%B4%E6%AD%A3%E5%88%99%E5%8C%96%E5%8F%82%E6%95%B0%EF%BC%89"><span class="nav-number">10.</span> <span class="nav-text">Adjusting the regularization parameter（调整正则化参数）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Selecting-%CE%BB-using-a-cross-validation-set%EF%BC%88%E4%BD%BF%E7%94%A8%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E9%9B%86%E9%80%89%E6%8B%A9%CE%BB%EF%BC%89"><span class="nav-number">11.</span> <span class="nav-text">Selecting λ using a cross validation set（使用交叉验证集选择λ）</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="yhellow"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">yhellow</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">303</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">156</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yhellow</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">4.3m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">65:01</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
