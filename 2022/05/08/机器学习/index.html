<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="监督学习监督学习是从“标记”的训练数据来推断一个功能的机器学习任务，训练数据包括一套训练示例  在监督学习中，每个实例都是由一个输入对象（通常为矢量）和一个期望的输出值（也称为监督信号）组成 监督学习算法是分析该训练数据，并产生一个推断的功能，其可以用于映射出新的实例 一个最佳的方案将允许该算法来正确地决定那些看不见的实例的类标签 这就要求学习算法是在一种“合理”的方式从一种从训练数据到看不见的情">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习（持续更新）">
<meta property="og:url" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Pwn进你的心">
<meta property="og:description" content="监督学习监督学习是从“标记”的训练数据来推断一个功能的机器学习任务，训练数据包括一套训练示例  在监督学习中，每个实例都是由一个输入对象（通常为矢量）和一个期望的输出值（也称为监督信号）组成 监督学习算法是分析该训练数据，并产生一个推断的功能，其可以用于映射出新的实例 一个最佳的方案将允许该算法来正确地决定那些看不见的实例的类标签 这就要求学习算法是在一种“合理”的方式从一种从训练数据到看不见的情">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651726810083-1653371283126.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651754207050-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651754218878-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651675596794-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651675709683-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651830266751-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651831283984-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Users/ywx813/Desktop/机器学习/机器学习/1651831283984.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651890724535-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651891357766-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651931216147-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651932054305-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651914301836-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651914279033-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651936037264-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651937189755-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651938047847-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652023019404-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652022966440-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652023900384-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652027285928-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652028011019-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652028442786-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652103923204-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652105282520-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652104965954-1653371283127.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652103678228-1653371283128.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652106436568-1653371283128.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652106658030-1653371283128.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652106616884-1653371283128.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652180531195-1653371283128.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652181824906-1653371283128.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652183667886-1653371283128.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652188426080-1653371283128.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652533210554-1653371283128.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652588451025-1653371283128.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652706307484-1653371283128.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652708323393-1653371283128.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652710284816-1653371283128.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652710566556-1653371283128.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652710775037-1653371283128.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652875817979.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652876615894.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652876762133.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652878030260.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652882030322.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652883071042.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1653151038604.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1653152301671.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1653205137538.png">
<meta property="og:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1653205186620.png">
<meta property="article:published_time" content="2022-05-07T17:34:20.000Z">
<meta property="article:modified_time" content="2022-05-24T05:48:12.753Z">
<meta property="article:author" content="yhellow">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651726810083-1653371283126.png">

<link rel="canonical" href="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>机器学习（持续更新） | Pwn进你的心</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Pwn进你的心</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yhellow">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Pwn进你的心">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习（持续更新）
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-05-08 01:34:20" itemprop="dateCreated datePublished" datetime="2022-05-08T01:34:20+08:00">2022-05-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-24 13:48:12" itemprop="dateModified" datetime="2022-05-24T13:48:12+08:00">2022-05-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>16k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>15 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><p>监督学习是从“标记”的训练数据来推断一个功能的机器学习任务，训练数据包括一套训练示例</p>
<ul>
<li>在监督学习中，每个实例都是由一个输入对象（通常为矢量）和一个期望的输出值（也称为监督信号）组成</li>
<li>监督学习算法是分析该训练数据，并产生一个推断的功能，其可以用于映射出新的实例</li>
<li>一个最佳的方案将允许该算法来正确地决定那些看不见的实例的类标签</li>
<li>这就要求学习算法是在一种“合理”的方式从一种从训练数据到看不见的情况下形成</li>
</ul>
<p>监督学习可以分为两类：回归问题，分类问题</p>
<ul>
<li>回归问题：通过给定一组数据，我们想要预测出连续的数值输出</li>
<li>分类问题：我们没法预测一个离散值输出（对或者错，“0”或者“1”，当然也可能是多个确定的选项），只能推测一种结果的可能性</li>
</ul>
<h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><p>在监督学习中，程序被明确告知了什么是“正确”，而在无监督学习中，训练数据没有任何的“标记”，而程序会尝试把这些训练数据分为：有着某些相同性质的簇（聚类算法）</p>
<ul>
<li>无监督学习即没有标注的训练数据集，需要根据样本间的统计规律对样本集进行分析，常常被用于数据挖掘，用于在大量无标签数据中发现规律</li>
<li>而聚类是无监督学习的常见任务，就是将观察值聚成一个一个的组，每一个组都含有一个或者几个特征，‎聚类的目的在于‎‎把相似的东西聚在一起，而我们并不关心这一类是什么‎‎</li>
<li>因此，一个聚类算法通常只需要知道‎‎如何计算相似度‎‎就可以开始工作了</li>
<li>‎例如无监督学习应该能在不给任何额外提示的情况下，仅依据一定数量的“狗”的图片特征，将“狗”的图片从大量的各种各样的图片中将区分出来</li>
</ul>
<h2 id="线性回归-回归"><a href="#线性回归-回归" class="headerlink" title="线性回归-回归"></a>线性回归-回归</h2><p>线性回归分析（Linear Regression Analysis）是确定两种或两种以上 <strong>变量间相互依赖的定量关系</strong> 的一种 <strong>统计分析方法</strong></p>
<p>例如：</p>
<ul>
<li>身高：由父亲的身高、母亲的身高、家庭收入、所在地区等因素决定</li>
<li>房价：由地段、面积、周围配套、时间等因素决定</li>
</ul>
<p>线性回归要做的是就是找到一个数学公式能相对较完美地把所有自变量组合（加减乘除）起来，得到的结果和目标越接近越好（也就是说，代价函数越小越好）</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651726810083-1653371283126.png" class width="1651726810083"> 
<ul>
<li>线性回归本质上就是把“散点图”拟合为一条直线，有许多种方法可以实现这个操作</li>
<li>其中最常见的算法就是梯度下降，梯度下降可以是代价函数到达最下值</li>
</ul>
<h2 id="多项式回归-回归"><a href="#多项式回归-回归" class="headerlink" title="多项式回归-回归"></a>多项式回归-回归</h2><p>如果您的数据点显然不适合线性回归（通过所有数据点的直线），则多项式回归可能是理想选择</p>
<p>多项式回归与线性回归一样，使用变量 x 和 y 之间的关系来找到通过数据点画线的最佳方法</p>
<p>案例：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651754207050-1653371283127.png" class width="1651754207050"> 
<script type="math/tex; mode=display">
y = ax + b</script><img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651754218878-1653371283127.png" class width="1651754218878"> 
<script type="math/tex; mode=display">
y = ax^2 + bx + c</script><p>我们可以从另外一个角度来理解这个式子，如果把 “x的平方” 理解成一个特征，把 “x” 理解成另外一个特征，这样这式子依然是线性回归的式子，但从 “x” 的角度来看是一个非线性的方程</p>
<p>这样的方式就叫多项式回归，相当于我们为样本多添了特征，这些特征是原来样本的多项式项</p>
<h2 id="均方误差-代价函数"><a href="#均方误差-代价函数" class="headerlink" title="均方误差-代价函数"></a>均方误差-代价函数</h2><p>首先我们需要知道：代价是什么？</p>
<ul>
<li>可以简单的把“代价”理解为“误差”</li>
<li>对于机器学习的目标，无非也就是最小化误差，也就是让代价最小化</li>
</ul>
<p>假设有训练样本 (x, y)，模型为 h，参数为 θ：</p>
<script type="math/tex; mode=display">
h_θ=θ^Tx（θ^T表示θ的转置）</script><p>概况来讲，任何能够衡量模型预测出来的值 h(θ) 与真实值 y 之间的差异的函数都可以叫做代价函数C(θ)，如果有多个样本，则可以将所有代价函数的取值求均值，记做J(θ)，因此可以得出以下关于代价函数的性质： </p>
<ul>
<li>对于每种算法来说，代价函数不是唯一的</li>
<li>代价函数是参数 θ 的函数</li>
<li>总之，代价函数 J(θ) 可以用来评价模型的好坏，代价函数越小说明模型和参数越符合训练样本 (x, y)</li>
<li>J(θ) 是一个标量</li>
</ul>
<p>代价函数（cost function）是用来衡量模型好坏的函数，我们的目标当然是得到最好的模型（也就是最符合训练样本的模型），因此训练参数的过程就是不断改变 θ，从而得到更小的 J(θ) 的过程，理想情况下，当我们取到代价函数 J 的最小值时，就得到了最优的参数 θ </p>
<p>均方误差（Mean squared error）是在线性回归中最常用的代价函数：</p>
<script type="math/tex; mode=display">
J(θ)=
\frac1{2m}\sum_{i=1}^m(\hat y^{(i)}−y^{(i)})^2=
\frac1{2m}\sum_{i=1}^m(h_θ(x^{(i)})−y^{(i)})^2</script><ul>
<li>m：训练样本的个数</li>
<li>hθ(x)：用参数θ和x预测出来的y值</li>
<li>y：原训练样本中的y值，也就是标准答案</li>
<li>上角标(i)：第i个样本</li>
</ul>
<p>适用的线性方程为：（其实就是一次函数）</p>
<script type="math/tex; mode=display">
h_θ=θ_0+θ_1x(适用的线性方程)</script><p>注意：在实际的操作中，代价函数处理返回 cost(代价) 以外，还需要返回 grad(梯度)，也就是当前代价的偏导数（可以去看看后续的实验操作中的代价函数，它们都是返回这两个值的）</p>
<ul>
<li>相应地，线性回归的代价对 θj 的偏导数定义为：（无正则化）</li>
</ul>
<script type="math/tex; mode=display">
\frac{∂J(θ)}{∂θ_0}=\frac{1}{m}\sum_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_{j}^{(i)}</script><h2 id="梯度下降-拟合函数"><a href="#梯度下降-拟合函数" class="headerlink" title="梯度下降-拟合函数"></a>梯度下降-拟合函数</h2><p>梯度下降用于将函数 J（代价函数）最小化，其核心思想是通过求梯度的方法来找到代价函数的最小值</p>
<ul>
<li>假设我们有一个需要最小化的函数 J(θ) ，我们需要做的就是从“0”开始一点一点改变 θ（如果有 θ1，θ2 …. 也要一起修改），直到我们找到 J(θ) 的最小值</li>
<li>当然不是随意修改 θ，每次都要在原来的基础上选择下降最快的方式</li>
</ul>
<p>第一次操作：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651675596794-1653371283127.png" class width="1651675596794"> 
<p>第二次操作：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651675709683-1653371283127.png" class width="1651675709683"> 
<p>可以发现：两次操作都是在当前的基础上获取最优解，但是结果却截然不同，这就是梯度下降的特点之一，下面看看梯度下降具体的数学原理：（为了方便理解，这里只设置了两个参数θ）</p>
<script type="math/tex; mode=display">
repeat\,\,\, until\,\,\, convergence\{ 
\\
θ_j:=θ_j-\alpha\frac∂{∂θ_j}J(θ_0,θ_1)\quad(for\,\,\,j=0\,\,\,and\,\,\,j=1)
\\
\}</script><p>反复执行这个过程，直至收敛</p>
<ul>
<li>“:=” 用来表示赋值，相当于C语言中的“=”</li>
<li>“α”（学习率）用于控制每次梯度下降时需要迈出的步幅</li>
<li>“θ” 用于表示参数（案例中就是代表了x，y），每次需要同时同步更新各个“θ”</li>
</ul>
<p>正确的更新方式：</p>
<script type="math/tex; mode=display">
temp0:=θ_0-\alpha\frac∂{∂θ_0}J(θ_0,θ_1)\\
temp1:=θ_1-\alpha\frac∂{∂θ_1}J(θ_0,θ_1)\\
θ_0:=temp0\\
θ_1:=temp1\\</script><p>错误的更新方式：</p>
<script type="math/tex; mode=display">
temp0:=θ_0-\alpha\frac∂{∂θ_0}J(θ_0,θ_1)\\
θ_0:=temp0\\
temp1:=θ_1-\alpha\frac∂{∂θ_1}J(θ_0,θ_1)\\
θ_1:=temp1\\</script><h2 id="正规方程-拟合函数"><a href="#正规方程-拟合函数" class="headerlink" title="正规方程-拟合函数"></a>正规方程-拟合函数</h2><p>为了获取代价函数的最小值，我们采用了梯度下降这种迭代的方式，分多次进行求解，而正规方程只需要运算一次就可以获取最小值</p>
<p>如果我们要获取一个函数最小值，最常见的办法就是对它求导，如果函数的最高次很高的话，方程就会很复杂，使用算法难以实现，而正规方程利用矩阵完成了这个过程</p>
<p>PS：本人太菜，理解不了正规方程</p>
<h2 id="特征规范化-优化"><a href="#特征规范化-优化" class="headerlink" title="特征规范化-优化"></a>特征规范化-优化</h2><p>不同特征具有不同量级时会导致：</p>
<ul>
<li>数量级的差异将导致量级较大的特征占据主导地位</li>
<li>数量级的差异将导致迭代收敛速度减慢（收敛过程会来回偏转）</li>
<li>依赖于样本距离的算法对于数据的数量级非常敏感</li>
</ul>
<p>所以，当特征相差几个数量级时，首先执行特征缩放可以使梯度下降更快地收敛，另外，特征的值过大或过小也会导致收敛速度下降</p>
<p>特征缩放：我们可以对特征进行“乘除”操作，来使其到达合适的范围</p>
<ul>
<li>从数据集中减去每个特征的平均值</li>
<li>减去平均值后，再将特征值按各自的“标准偏差”进行缩放（除）</li>
</ul>
<h2 id="逻辑回归-分类"><a href="#逻辑回归-分类" class="headerlink" title="逻辑回归-分类"></a>逻辑回归-分类</h2><p>Logistic Regression 虽然被称为回归，但其实际上是分类模型，并常用于二分类，用于解决分类问题</p>
<p>下面是一些分类问题的例子：</p>
<ul>
<li>邮件垃圾分类</li>
<li>交易是否有欺诈</li>
</ul>
<p>通常，这些数据都只有两种或几种可能（有限次数）：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651830266751-1653371283127.png" class width="1651830266751"> 
<p>用线性回归来不能很好的拟合这些数据，因此逻辑回归诞生了</p>
<p>逻辑回归的本质是：假设数据服从这个分布，然后使用极大似然估计做参数的估计</p>
<h2 id="假设陈述-分类"><a href="#假设陈述-分类" class="headerlink" title="假设陈述-分类"></a>假设陈述-分类</h2><p>首先，我们需要我们的分类器输出值在 [ 0 , 1 ] 之间，因此，我们提出一个“假设”来满足该性质，假设的形式为：</p>
<script type="math/tex; mode=display">
h_θ=g(θ^Tx)</script><p>我们定义函数 g 为：</p>
<script type="math/tex; mode=display">
g(z)=\frac1{1+e^{-z}}</script><p>把它们组合一下，可以得到：</p>
<script type="math/tex; mode=display">
h_θ=\frac1{1+e^{-θ^Tx}}</script><p>这就是 Sigmoid 函数（另外，sigmoid 函数和 logistic 函数是同一个意思，完全可以相互替换），它的图形大概是这个样子：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651831283984-1653371283127.png" class width="1651831283984"> 
<p>当假设输出某个数字时，我们会把这个数字当做：对于输入值“x”，“y=1”的概率估计</p>
<p>例如：假设一个学校需要根据学生的成绩来录取学生，而我们拥有学生成绩这一数据集，于是我们把学生成绩作为“x”输入 Sigmoid 函数 h(x)，就可以得到该学生录取的概率</p>
<p>但是，我们仍然需要利用各种拟合函数使 θ 最小化，这样 Sigmoid 函数才有意义</p>
<h2 id="决策边界-分类"><a href="#决策边界-分类" class="headerlink" title="决策边界-分类"></a>决策边界-分类</h2><p>理解决策边界（Decision Boundary），可以帮助我们了解 Sigmoid 函数到底在计算什么</p>
<p>Sigmoid 函数的图形：</p>
<p><img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Users/ywx813/Desktop/机器学习/机器学习/1651831283984.png" alt="1651831283984"> </p>
<p>我们发现：</p>
<ul>
<li>当 z &lt; 0 时，h(x) &lt; 0.5（“z”就是函数 g(z) 的参数）</li>
<li>当 z &gt; 0 时，h(x) &gt; 0.5（“z”就是函数 g(z) 的参数）</li>
</ul>
<p>所以点 (0 , 0.5) 就是“决策边界”的判定点：</p>
<ul>
<li>如果 “h(x) &lt; 0.5” ，z &lt; 0，我们判断 “y=0” </li>
<li>如果 “h(x) &gt; 0.5” ，z &gt; 0，我们判断 “y=1” </li>
<li>如果 “h(x) = 0.5” ，预测为正类（我们不用在意）</li>
</ul>
<p>于是判断样本是否为某个“可能”的条件，从 “h(x) vs 0.5” 转化为了 “z vs 0”，这里的 z 就是我们需要拟合的模板函数（要根据数据集散点图判断该函数的形状，然后添加对应的多项式），当拟合完毕后，z 就是“决策边界”</p>
<p>案例一：</p>
<p>假设我们有如下一组数据集：（多特征）</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651890724535-1653371283127.png" class width="1651890724535">  
<p>并且我们已经拟合好了数据（利用“拟合函数”使“代价函数”最小），得到一组固定的 θ [-3,1,1] </p>
<p>用直线 -3+x1+x2 = 0 作为“决策边界”：</p>
<ul>
<li>如果 “ -3+x1+x2 &gt; 0 ” ，我们判断 “y=1” </li>
<li>如果 “ -3+x1+x2 &lt; 0 ” ，我们判断 “y=0” </li>
<li>如果 “ -3+x1+x2 &lt; 0 ” ，预测为正类</li>
</ul>
<p>案例二：</p>
<p>假设我们有如下一组数据集：（增添两个格外特征）</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651891357766-1653371283127.png" class width="1651891357766"> 
<p>并且我们已经拟合好了数据，得到一组固定的 θ [-1,0,0,1,1] </p>
<p>用圆 -1+(x1)^2+(x2)^2 = 0 作为“决策边界”：</p>
<ul>
<li>-1+(x1)^2+(x2)^2 &gt; 0，我们判断 “y=1” </li>
<li>-1+(x1)^2+(x2)^2 &lt; 0，我们判断 “y=0” </li>
<li>-1+(x1)^2+(x2)^2 = 0，预测为正类</li>
</ul>
<p>通过在特征中增加这些复杂的多项式，我们可以得到更复杂的决策边界</p>
<p>注意：决策边界不是训练集的属性，只要给定了 θ ，决策边界就确定了，而不是用训练集来定义决策边界</p>
<h2 id="多元分类-分类"><a href="#多元分类-分类" class="headerlink" title="多元分类-分类"></a>多元分类-分类</h2><p>在实际问题中，我们可能会遇到有“多种可能”的特征</p>
<p>例如：</p>
<ul>
<li>邮件分类：家人，同学，诈骗……</li>
<li>考试等级：A，B，C，D……</li>
</ul>
<p>接下来我们就来讨论遇到多元分类时的处理办法，假设有一个三元分类：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651931216147-1653371283127.png" class width="1651931216147"> 
<p>我们可以把这个三元分类分为三个二元分类：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651932054305-1653371283127.png" class width="1651932054305"> 
<p>可以把另外两个分类定义为新的“伪训练集”，然后按照二元分类的方式进行拟合</p>
<p>最后，我们有三个分类器，每个分类器都针对其中一种情况进行训练，得到对应分类“P==1”成立的概率，分类器通式如下：</p>
<script type="math/tex; mode=display">
h_θ^(x)=P(y=i|x;θ)\,\,\,\,\,\,(i=1,2,3)</script><p>意义为：当给定了 “x” 和 “θ” 时，“y = i” 的概率，虽然这三者的概率加起来可能不为“1”，但是我们并不关心，我们只需要选择概率最高的那个就可以了</p>
<h2 id="交叉熵-代价函数"><a href="#交叉熵-代价函数" class="headerlink" title="交叉熵-代价函数"></a>交叉熵-代价函数</h2><p>先看看回归问题常用的代价函数：<strong>均方误差</strong>（做了一些变化）</p>
<script type="math/tex; mode=display">
J(θ)=\frac1{m}\sum_{i=1}^m(\frac1{2}h_θ(x^{(i)})−y^{(i)})^2</script><p>我们把关键的部分提取出来：</p>
<script type="math/tex; mode=display">
J(θ)=\frac1{m}\sum_{i=1}^mCost(h_θ(x^{(i)}),y^{(i)})\\
Cost(h_θ(x^{(i)}),y^{(i)})=\frac1{2}(h_θ(x^{(i)})-y^{(i)})^2</script><p>问题的关键就是 h(x) 函数（模型），线性回归和逻辑回归的 h(x) 函数是不同的：</p>
<p>线性回归：</p>
<ul>
<li>公式</li>
</ul>
<script type="math/tex; mode=display">
h_θ=θ^Tx</script><ul>
<li>图像</li>
</ul>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651914301836-1653371283127.png" class width="1651914301836"> 
<p>逻辑回归：</p>
<ul>
<li>公式</li>
</ul>
<script type="math/tex; mode=display">
h_θ=\frac1{1+e^{-θ^Tx}}</script><ul>
<li>图像</li>
</ul>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651914279033-1653371283127.png" class width="1651914279033"> 
<p>可以发现：h(x) 函数的改变导致“代价函数-均方误差”变为了非凸函数，使梯度下降算法不容易找到代价函数的最小值（可能找到一个极小值点就停了），这样会导致梯度下降的拟合性很差</p>
<p>为了解决这个问题，逻辑回归会使用适用于它自己的代价函数-<strong>交叉熵</strong></p>
<script type="math/tex; mode=display">
J(θ)=\frac1{m}\sum_{i=1}^mCost(h_θ(x^{(i)}),y^{(i)})
=-\frac1{m}[\sum_{i=1}^my^{(i)}log\,h_θ(x^{(i)})+(1-y^{(i)})log(1-h_θ(x^{(i)}))]</script><h2 id="fminunc-拟合函数"><a href="#fminunc-拟合函数" class="headerlink" title="fminunc-拟合函数"></a>fminunc-拟合函数</h2><p>fminunc 采用拟牛顿法 (QN)，是一种使用导数的算法，优化工具箱提供 fmincon 函数用于对有约束优化问题进行求解</p>
<p>Octave/MATLAB 的 fminunc 是一个优化解算器，可以找到无约束函数的最小值，对于逻辑回归，需要优化成本函数 J(θ)，具体来说，您将使用 fminunc 找到最佳参数 θ 对于逻辑回归成本函数 J(θ)</p>
<p>在 Python 中也有该函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.optimize <span class="comment"># SciPy的optimize模块提供了常用的最优化算法函数实现</span></span><br><span class="line"></span><br><span class="line">theta, cost, *unused = opt.fmin_bfgs(f=cost_func, fprime=grad_func, x0=init_theta, maxiter=<span class="number">400</span>, full_output=<span class="literal">True</span>, disp=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h2 id="过拟合问题-优化"><a href="#过拟合问题-优化" class="headerlink" title="过拟合问题-优化"></a>过拟合问题-优化</h2><p>我们还是以“预测房子的价格”为案例：</p>
<ul>
<li>房子的价格和房子的大小有关</li>
</ul>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651936037264-1653371283127.png" class width="1651936037264"> 
<p>现在根据拟合程度的不同，有三种情况：A（左），B（中），C（右）</p>
<ul>
<li>随着房子的面积增大，房子的价格趋于平缓，所以 A 不能很好的拟合该模型（称为欠拟合）</li>
<li>在 B 中加入了一个2次多项式，使拟合效果很好</li>
<li>而 C 中加入了一个3次多项式，一个4次多项式，在现有的数据集中也许可以很好的拟合，但是总体来说不符合实际，我们也没有更多的数据来约束它（称为过拟合）</li>
</ul>
<p>这种过度拟合的问题会在参数θ变多的情况下发生：</p>
<ul>
<li>因为能更好的拟合现有的数据，所以随着参数θ增多， J(θ)（代价函数）会不断的接近“0”，但是拟合曲线也会越来越扭曲</li>
<li>拟合曲线会千方百计的拟合数据集，如果没有足够的数据来约束它，它就可能无法泛化到新的样本中，导致无法预测数据</li>
</ul>
<p>下面这个逻辑回归的例子可能会明显些：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651937189755-1653371283127.png" class width="1651937189755">  
<p>解决办法：</p>
<ul>
<li>数据可视化：绘制决策边界的图像，初步判断参数θ的数目合不合适</li>
<li>人工调查选择：在数据比较复杂的模型中，绘图是比较困难的而且往往解决不了问题（有时即使有图形，也不好判断是否过拟合），所以需要人工排查哪些变量比较重要，那些可以去除，尽可能减少变量</li>
<li>模型选择算法：可以自动选择哪些特征变量可以保留，哪些可以去除（但是这种算法不可控，可能会因为舍弃关键变量，而导致拟合程度下降）</li>
<li>正则化：我们将保留所有特征变量，但是减少量级，或者参数θ的大小</li>
</ul>
<h2 id="正则化-优化-代价函数"><a href="#正则化-优化-代价函数" class="headerlink" title="正则化-优化-代价函数"></a>正则化-优化-代价函数</h2><img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1651938047847-1653371283127.png" class width="1651938047847"> 
<p>在之前的案例中：增加参数θ的数目会导致 <strong>过拟合</strong>，使泛化效果不好</p>
<p>如果我们在函数中加入“惩罚项”，使“θ3”，“θ4”变得很小，这就可以平衡过拟合的影响</p>
<p><strong>正则化-均方误差</strong></p>
<script type="math/tex; mode=display">
J(θ)=\frac1{2m}[\sum_{i=1}^m(h_θ(x^{(i)})−y^{(i)})^2+λ\sum_{j=1}^nθ_j^{2}]</script><p>其实就是在标准均方误差上面加了一项（惩罚项，正则化项），使其缩小所有的参数（因为我们不知道要优化哪些参数），这里的 λ 就是正则化参数，用于控制各个参数之间的平衡</p>
<p>PS：之后在重选择中 ，会有很多方法来自动选择 λ</p>
<p><strong>正则化-交叉熵</strong></p>
<script type="math/tex; mode=display">
J(θ)=-\frac1{m}[\sum_{i=1}^my^{(i)}log\,h_θ(x^{(i)})+(1-y^{(i)})log(1-h_θ(x^{(i)}))]
+\frac{λ}{2m}\sum_{j=1}^nθ_j^{2}</script><h2 id="正则化-优化-拟合函数"><a href="#正则化-优化-拟合函数" class="headerlink" title="正则化-优化-拟合函数"></a>正则化-优化-拟合函数</h2><p>之前我们使用了梯度下降的方法来拟合模型，如果对代价函数进行了正则化，那么 <strong>梯度下降</strong> 也要进行正则化才行</p>
<p><strong>适用于均方误差-梯度下降</strong></p>
<p>梯度下降通用公式：</p>
<script type="math/tex; mode=display">
repeat\,\,\, until\,\,\, convergence\{ 
\\
θ_j:=θ_j-\alpha\frac∂{∂θ_j}J(θ)\quad(j=0\,\,,1\,\,,2\,......)
\\
\}</script><p>把 J(θ) 替换为均方误差：</p>
<script type="math/tex; mode=display">
repeat\,\,\, until\,\,\, convergence\{ 
\\
θ_j:=θ_j-\alpha\frac1{m}\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}
\quad(j=0\,\,,1\,\,,2\,......)
\\
\}</script><p>进行正则化：</p>
<script type="math/tex; mode=display">
repeat\,\,\, until\,\,\, convergence\{ 
\\
θ_j:=θ_j-\alpha[\frac1{m}\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{λ}{m}θ_j]
\quad(j=1\,\,,2\,\,,3\,......)(我们约定正则从1开始)
\\
\}</script><p><strong>适用于交叉熵-梯度下降</strong></p>
<p>交叉熵通用公式：</p>
<script type="math/tex; mode=display">
J(θ)=-\frac1{m}[\sum_{i=1}^my^{(i)}log\,h_θ(x^{(i)})+(1-y^{(i)})log(1-h_θ(x^{(i)}))]</script><p>进行正则化：</p>
<script type="math/tex; mode=display">
J(θ)=-\frac1{m}[\sum_{i=1}^my^{(i)}log\,h_θ(x^{(i)})+(1-y^{(i)})log(1-h_θ(x^{(i)}))+\frac{λ}{2m}\sum_{j=1}^nθ_j^{2}]</script><p><strong>同时，求梯度的公式也需要正则化</strong>（通常的代价函数都会返回：cost(代价)，grad(梯度)）</p>
<p>求梯度通用公式：</p>
<script type="math/tex; mode=display">
\frac{∂J(θ)}{∂θ_0}=\frac{1}{m}\sum_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_{j}^{(i)}</script><p>进行正则化：（注意：正则化是从“1”开始，而不是“0”）</p>
<script type="math/tex; mode=display">
\frac{∂J(θ)}{∂θ_0}=\frac{1}{m}\sum_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_{j}^{(i)}\quad for\,\,j=0\\
\frac{∂J(θ)}{∂θ_0}=(\frac{1}{m}\sum_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_{j}^{(i)})+\frac{λ}{m}θ_j\quad for\,\,j>0</script><h2 id="向量化-优化-代价函数"><a href="#向量化-优化-代价函数" class="headerlink" title="向量化-优化-代价函数"></a>向量化-优化-代价函数</h2><p>向量化是非常基础的去除代码中 <strong>for</strong> 循环的艺术</p>
<ul>
<li>当在深度学习安全领域、深度学习实践中应用深度学习算法时，会发现在代码中显式地使用 for 循环使算法很低效</li>
<li>所以算法能应用且没有显式的 for 循环是很重要的，并且会帮助你适用于更大的数据集</li>
</ul>
<p>在深度学习领域这里有一项叫做向量化的技术，是一个关键的技巧，它可以允许你的代码摆脱这些显式的 <strong>for</strong> 循环，举个栗子说明什么是向量化：</p>
<p>在逻辑回归中，需要去计算 z = (w^T)x + b（其中 w，x 都是列向量），如果有很多的特征，那么就会有一个非常大的向量，那么如果想使用非向量化方法去计算 (w^T)x ，就需要如下的方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_x):</span><br><span class="line">    z += w[i] * x[i]</span><br><span class="line">z += b</span><br></pre></td></tr></table></figure>
<p>可以发现非向量化的实现有 for 循环，作对对比，向量化的实现将会直接计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z = np.dot(w, x) + b</span><br></pre></td></tr></table></figure>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>神经网络产生的最初目的是为了制造出模拟人脑的机器，下面是我们神经元的结构图：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652023019404-1653371283127.png" class width="1652023019404"> 
<p>神经元通过许多树突来接收电信号，在细胞体中处理过后，又通过轴突输出电信号，我们可以用以下这个模型来模拟这个过程：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652022966440-1653371283127.png" class width="1652022966440"> 
<p>x1，x2，x3 通过“树突”传输到运算函数 hθ(x)，然后通过“轴突”输出数据</p>
<p>这里的 hθ(x) 就可以是逻辑回归中的 Sigmoid 函数（假设陈述），而 θ 就是模型的参数向量（在神经网络中也被称为“权重”）</p>
<script type="math/tex; mode=display">
h_θ=\frac1{1+e^{-θ^Tx}}</script><p>而这个模型就被称为：带有 Sigmoid 激活函数的人工神经元</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652023900384-1653371283127.png" class width="1652023900384"> 
<p>神经网络其实就是一组神经元连接在一起的集合，第一层被称为“输入层”，第二层被称为“隐藏层”，最后一层被称为“输出层”（其实除了第一层和最后一层，其他层都是“隐藏层”）</p>
<p>接下来来分析一下神经网络的计算流程：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652027285928-1653371283127.png" class width="1652027285928"> 
<ul>
<li>假设我们输入 x1，x2，x3 到“隐藏层”，“隐藏层”根据自己的算法输出 a1，a2，a3，然后这3个数据作为最终“输出层” hθ(x) 的参数，计算出最终的结果</li>
<li>每个样本都会根据当前 θ 拟合出的函数模型进行计算，输出一个预测的结果，然后把预测结果同真实数据进行对比，通过一些算法来调整 θ，使其结果更加趋近于真实值</li>
<li>当所有数据都处理完毕以后，每个节点都大概拥有了一个相对稳定的 θ 值，然后就可以预测数据了</li>
</ul>
<p>接下来我们把视角聚集到某个节点：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652028011019-1653371283127.png" class width="1652028011019"> 
<ul>
<li>我们把上述模型中的“输入层”屏蔽，后续的“隐藏层”和“输出层”其实构成了一个“逻辑回归模型”（只不过这里不直接传入数据集，而是传入“上一层”的运算结果）</li>
<li>对这一个节点进行拟合，就可以得到一组确定的参数 θ</li>
</ul>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652028442786-1653371283127.png" class width="1652028442786"> 
<ul>
<li>而把整个神经网络都拟合完毕以后，每个节点都可能有一组不同的参数 θ（每个节点都输出它们认为的“最佳值”，到最后的“输出层”就可以输出该样本最可能的类）</li>
</ul>
<p>神经网络的连接方式也被称为神经网络架构，通过改变架构，就可以模拟出更加复杂的特征（相比于逻辑回归中，通过增加参数 θ 的数目来模拟复杂的特征，这种方式更为高效）</p>
<p><strong>案例一：使用神经网络来模拟 AND（OR同理）</strong></p>
<p>假设我们有两个 x1，x2 两个输入特征，它们只能取“0”或者“1”</p>
<p>现在 y = x1 AND x2（和“&amp;&amp;”一样，当“x1==x2”时，“y=1”，否则“y=0”），它的神经网络模型如下：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652103923204-1653371283127.png" class width="1652103923204"> 
<ul>
<li>PS：这个“+1”被称为“+1单元”，是我们引入的值</li>
</ul>
<p>经过拟合之后的权重结果为：θ [-30,20,20]，也就意味着该模型为：</p>
<script type="math/tex; mode=display">
h_θ^{(x)}=g(-30+20x_1+20x_2)</script><p>解释一下该模型： </p>
<ul>
<li>g(x) 就是 Sigmoid 函数</li>
</ul>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652105282520-1653371283127.png" class width="1652105282520">  
<ul>
<li>x1 和 x2 就只有两种可能：“0”，“1”</li>
</ul>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652104965954-1653371283127.png" class width="1652104965954"> 
<ul>
<li>当 x1 == 0 &amp;&amp; x2 == 0 时，g(-30) ≈ 0</li>
<li>当 x1 == 0 &amp;&amp; x2 == 1 时，g(-10) ≈ 0</li>
<li>当 x1 == 1 &amp;&amp; x2 == 0 时，g(-10) ≈ 0</li>
<li>当 x1 == 1 &amp;&amp; x2 == 1 时，g(10) ≈ 1</li>
</ul>
<p>和逻辑运算符 AND 的逻辑基本相同，成功模拟出 AND（注意：这个权重 θ 是拟合出来的）</p>
<p><strong>案例二：使用神经网络来模拟 XNOR</strong></p>
<p>XNOR为“同或”，和“异或相反”：若相同则输出为“1”，不同则输出为“0”，它的图像如下：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652103678228-1653371283128.png" class width="1652103678228"> 
<ul>
<li>这是明显的非线性结构，使用单一的逻辑回归是解决不了问题的，不过使用神经网络就可以通过结点之间的组合来解决问题</li>
<li>学习这个模型，其实是想体验一下各个神经结点的组合搭配过程</li>
</ul>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652106436568-1653371283128.png" class width="1652106436568"> 
<p>“同或”其实就是以上这3个模型拼接出来的：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652106658030-1653371283128.png" class width="1652106658030"> 
<ul>
<li>红色的部分（-30,20,20）是 AND</li>
<li>蓝色的部分（10,-20,-20）是 AND-NOT</li>
<li>它们组合出来的“隐藏层”可以放入下表的 a1，a2：</li>
</ul>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652106616884-1653371283128.png" class width="1652106616884"> 
<ul>
<li>绿色的部分（-10,20,20）是 OR，发现“隐藏层”进行“或”操作后，刚好就可以得到 XNOR 的结果</li>
</ul>
<p>这个案例证明了神经网络的功能强大：通过改变架构，就可以模拟出更加复杂的特征</p>
<h2 id="交叉熵-神经网络-代价函数"><a href="#交叉熵-神经网络-代价函数" class="headerlink" title="交叉熵(神经网络)-代价函数"></a>交叉熵(神经网络)-代价函数</h2><p>先看看逻辑回归中，正则化的交叉熵公式：</p>
<script type="math/tex; mode=display">
J(θ)=-\frac1{m}[\sum_{i=1}^my^{(i)}log\,h_θ(x^{(i)})+(1-y^{(i)})log(1-h_θ(x^{(i)}))]+\frac{λ}{2m}\sum_{j=1}^nθ_j^{2}</script><p>对于神经网络，我们将不再只有一个逻辑回归输出单元，而是有 K 个</p>
<p>那么它的公式如下：（又长又复杂）</p>
<script type="math/tex; mode=display">
J(θ)=-\frac{1}{m}[\sum_{i=1}^{m}\sum_{k=1}^{K}y_k^{(i)}log(h_θ(x^{(i)}))_k+(1-y_k^{(i)})log(1-(h_θ(x^{(i)}))_k)]+
\frac{λ}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s1}\sum_{j=1}^{s1+1}(θ_{ji}^{(l)})^2</script><p>对于一个确定的（xi，yi），那么它的代价函数可以看做是：</p>
<script type="math/tex; mode=display">
cost(i)=y^{(i)}log(h_θ(x^{(i)}))+(1-y^{(i)})log(1-h_θ(x^{(i)}))</script><p>为了方便理解，也可以把代价函数看做是如下式子：</p>
<script type="math/tex; mode=display">
cost(i)≈(h_θ(x^{(i)})-y^{(i)})^2</script><p>类似于均方误差，代表了神经网络预测样本值的精确程度</p>
<h2 id="反向传播-优化"><a href="#反向传播-优化" class="headerlink" title="反向传播-优化"></a>反向传播-优化</h2><ul>
<li>梯度下降：是寻找代价函数最小值的一种方法</li>
<li>交叉熵(神经网络)：神经网络的代价函数，需要提供“梯度”</li>
<li>反向传播：是求解梯度的一种方法</li>
</ul>
<p><strong>我们先分析一下正向传播的过程</strong></p>
<p><strong>为了拟合神经网络，我们需要使用反向传播算法，所以我们先分析一下程序正向传播的过程</strong></p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652180531195-1653371283128.png" class width="1652180531195"> 
<ul>
<li>a1 就是第一层的激活值，因其在输入层，我们假设它为 x</li>
<li>然后计算模型函数 z(x) 的值，把计算结果放入 Sigmoid 激活函数，计算出另一个激活值 a2</li>
<li>接着进行两次前向传播，分别计算出 a3，a4</li>
<li>最后的 a4 就是假设函数 h(x) 的输出</li>
</ul>
<p>案例一：（直观的展示了某个神经结点的运算过程）</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652181824906-1653371283128.png" class width="1652181824906"> 
<ul>
<li>注意紫色标注出来的 z1（模型函数）：</li>
</ul>
<script type="math/tex; mode=display">
z^{(3)}_{1}=θ^{(2)}_{10}*1+θ^{(2)}_{11}*a^{(2)}_{1}+θ^{(2)}_{12}*a^{(2)}_{1}</script><p>对于正向传播而言，我们采用梯度下降来拟合函数，但是在神经网络中，参数的数量是一个可怕的数字，动辄上万，十几万，并且，其取值有时是十分灵活的，甚至精确到小数点后若干位，若使用穷举法，将会造成一个几乎不可能实现的计算量</p>
<p>解决的办法就是：反向传播算法</p>
<p>采用反向传播算法来计算各个神经网络节点的梯度，并把它提供给代价函数（通常是交叉熵）</p>
<p><strong>接下来就来分析反向传播算法</strong></p>
<p>从直观上来说，反向传播算法就是对每一个结点进行一次这样的运算：</p>
<script type="math/tex; mode=display">
δ^{(l)}_{j}=a^{(l)}_{j}-y_j\,\,\,\,\,\,(l为Layer,代表神经网络的层数)</script><ul>
<li>δ( l , j ) 代表了第 l 层的第 j 结点的误差</li>
</ul>
<p>案例：对如下这个图像进行反向传播算法</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652183667886-1653371283128.png" class width="1652183667886"> 
<p>计算公式为：</p>
<script type="math/tex; mode=display">
For\,each\,output\,unit(layer\,\,L=4)\\
δ^{(4)}=a^{(4)}-y\\
δ^{(3)}=(θ^{(3)})^{T}δ^{(4)}.*g^{'}(z^{(3)})\\
δ^{(2)}=(θ^{(3)})^{T}δ^{(3)}.*g^{'}(z^{(2)})</script><ul>
<li>对于最后一层（输出层），δ4 就是 a4（输出层预测的结果）和 y（真实的结果）之间的差值</li>
<li>而对于中间的隐藏层，因为不清楚“预测结果”和“真实结果”的具体值，所以就只能通过以上的公式进行模拟计算</li>
<li>值得注意的是：<ul>
<li>“ .* ”对应了“点乘”，其结果会是一个标量</li>
<li>最后的结果运算到 δ2 就可以了，因为 δ1 就是真实的数据，没有误差</li>
</ul>
</li>
</ul>
<p>我们可以简单把 δ( l , j ) 理解为第 l 层的第 j 结点的误差，但它有更实际的意义：（需要高数基础，给跪了）</p>
<script type="math/tex; mode=display">
δ^{(l)}_{j}=\frac{∂}{∂z^{(j)}_{j}}cost(i)\\
cost(i)=y^{(i)}log(h_θ(x^{(i)}))+(1-y^{(i)})log(1-h_θ(x^{(i)}))\,\,\,\,\,\,
(在前面的交叉熵中已经说明)</script><ul>
<li>在神经网络内部，只要稍微改动一下 z(l)，就会影响到 cost（某个结点的代价），从而影响整个代价函数的值</li>
<li>所以 δ( l , j ) 其实是用于衡量：为了影响这些中间值（中间层的运算结果），将要改变多少权重（改变权重的程度）</li>
</ul>
<p>案例二：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652188426080-1653371283128.png" class width="1652188426080"> 
<p>假设我们已经计算出了 δ(3).1，δ(3).2，接下来我们要计算 δ(2).2</p>
<p>这个过程其实就是正向传播反了过来：</p>
<ul>
<li>列出节点 z(2).2 输出的两个权重：θ(2).12 和 θ(2).22 </li>
<li>列出节点 z(3).1 和 z(3).2 的δ值：δ(3).1 和 δ(3).2</li>
<li>通过以下公式进行计算：</li>
</ul>
<script type="math/tex; mode=display">
δ^{(2)}_{2}=θ^{(2)}_{12}δ^{(3)}_{1}+θ^{(2)}_{22}δ^{(3)}_{2}</script><h2 id="梯度检测-优化"><a href="#梯度检测-优化" class="headerlink" title="梯度检测-优化"></a>梯度检测-优化</h2><p>神经网络有一个不好的性质，那就是它容易产生BUG</p>
<p>当它也梯度下降或者其他算法一起工作时，也许它看起来确实能正常运行，但是反向传播的过程可能会因为一些BUG导致效率下降，因而使最后的结果和没有BUG时差出几个量级，更致命的是，我们可能根本就不知道发生了BUG</p>
<p>这个问题的解决办法就是<strong>梯度检测</strong>，原理如下：</p>
<ul>
<li>梯度检测会估计梯度（导数）值，然后和你程序计算出来的梯度的值进行对比，以判断程序算出的梯度值是否正确</li>
</ul>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652533210554-1653371283128.png" class width="1652533210554"> 
<ul>
<li>上图中，我们关注θ0点的函数的导数，即θ0点切线（图中蓝线）的斜率，现在我们在“θ0−ε”和“θ0+ε”两点连一条线（图中红线），我们发现红线的斜率和蓝线斜率很相似</li>
<li>红线的斜率可以用以下式子表示： </li>
</ul>
<script type="math/tex; mode=display">
\frac{J(θ_{0}+ε)-J(θ_{0}-ε)}{2ε}</script><ul>
<li>实际上，以上的式子很好地表示了θ0点导数的近似值</li>
<li>在实际的应用中，θ往往是一个向量，梯度下降算法要求我们对向量中的每一个分量进行偏导数的计算，对于偏导数，我们同样可以用以下式子进行近似计算：</li>
</ul>
<script type="math/tex; mode=display">
\frac{J(θ_{1}+ε,θ_{2},θ_{3},...,θ_{n})-J(θ_{1}-ε,θ_{2},θ_{3},...,θ_{n})}{2ε}</script><ul>
<li>梯度检测方法的开销是非常大的，比反向传播算法的开销都大，所以一旦用梯度检测方法确认了梯度下降算法算出的梯度（或导数）值是正确的，那么就及时关闭它</li>
<li>一般来说ε的值选“10的-4次方”，注意并不是越小越好</li>
</ul>
<h2 id="随机初始化-优化"><a href="#随机初始化-优化" class="headerlink" title="随机初始化-优化"></a>随机初始化-优化</h2><p>对于一个高级算法，需要对它的 θ 集合进行初始化，通常的想法就是把它们初始化为“0”</p>
<p>这个在逻辑回归中是没有问题的，但是在训练神经网络的时候，因为权重都是“0”，导致所有节点的激活值都是一样的，最后使每次更新出来的 θ 都一样</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652588451025-1653371283128.png" class width="1652588451025"> 
<ul>
<li>如果把某一层的 θ 都初始化为“0”，那么在梯度下降更新 θ 时，有如下的等式成立：</li>
</ul>
<script type="math/tex; mode=display">
\frac{∂}{∂θ^{(1)}_{1}}j(θ)=\frac{∂}{∂θ^{(1)}_{2}}j(θ)</script><ul>
<li>例如：第一条蓝线的权重会更新为“学习率”乘左边的式子，而第二条蓝线的权重会更新为“学习率”乘右边的式子，因为“θ1”和“θ2”都初始化为“0”，所以这个等式是恒成立的</li>
<li>接下来的两条红线也会相等，同样，两条绿线也会相等</li>
</ul>
<p>所以我们需要对 θ 进行随机初始化：</p>
<ul>
<li>对于每一个 θ，我们需要将其初始化为一个范围在 [-ε,ε] 之间的随机值</li>
<li>ε 是一个很小的值，通常为“10的-4次方”</li>
</ul>
<h2 id="偏差方差-误差"><a href="#偏差方差-误差" class="headerlink" title="偏差方差-误差"></a>偏差方差-误差</h2><p>当一个算法拟合效果不佳时，只有两种情况：偏差较大，方差较大（也就是欠拟合，过拟合）</p>
<script type="math/tex; mode=display">
Error=IrreducibleError+Bias^{2}+Variance</script><ul>
<li>Error，误差：模型的计算值和真实数据之间的差值</li>
<li>Irreducible Error，不可避免的误差：刻画了当前任务任何算法所能达到的期望泛化误差的下限，即刻画了问题本身的难度</li>
<li>Bias，偏差：刻画了算法的拟合能力（Bias 高表示预测函数与真实结果相差很大，拟合程度太低）</li>
<li>Variance，方差：代表 “同样大小的不同数据集训练出的模型” 与 “这些模型的期望输出值” 之间的差异（Var高表示模型很不稳定，泛化效果差）</li>
</ul>
<h2 id="交叉验证-误差"><a href="#交叉验证-误差" class="headerlink" title="交叉验证-误差"></a>交叉验证-误差</h2><p>之前我们已经介绍了误差的产生和原理，接下来就来看看误差的检查方法：交叉验证</p>
<ul>
<li>基本思想是将数据分为两部分，一部分数据用来模型的训练，称为训练集</li>
<li>另外一部分用于测试模型的误差，称为验证集</li>
</ul>
<p>还是那个熟悉的例子：（这里就不赘述了）</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652706307484-1653371283128.png" class width="1652706307484">  
<p>首先需要补充几个概念：</p>
<ul>
<li>训练集：数据集中，用于训练模型的部分</li>
<li>验证集：数据集中，用于调整模型的超参数和用于对模型的能力进行初步评估的部分</li>
<li>测试集：数据集中，用来评估模最终模型泛化能力的部分</li>
</ul>
<p>下面一张图片将会展示“偏差”，“方差”的区别：（以上述案例为例）</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652708323393-1653371283128.png" class width="1652708323393"> 
<ul>
<li>J(θ)train 代表了训练集的误差：我们会很明显的发现，随着特征值的增加，函数的误差明显下降</li>
<li>J(θ)cv 代表了测试集的误差：和训练集的误差曲线不同，测试集的误差曲线反应了该模型的真实拟合水平</li>
</ul>
<p>从 J(θ)cv 中我们可以发现两个极点：</p>
<ul>
<li>左极点代表了：偏差较大，拟合程度不够（欠拟合）</li>
<li>右极点代表了：方差较大，泛化效果不足（过拟合）</li>
</ul>
<h2 id="学习曲线-误差"><a href="#学习曲线-误差" class="headerlink" title="学习曲线-误差"></a>学习曲线-误差</h2><p>如果你想检测你的模型是否可以正常运行，那么学习曲线就是一种很好的工具</p>
<p>学习曲线的基本原理：人为减少训练集的数量，观察模型的误差曲线</p>
<script type="math/tex; mode=display">
J_{train}(θ)=\frac{1}{2m}\sum^{m}_{i=1}(h_θ(x^{(i)})-y^{(i)})^{2}\\
J_{cv}(θ)=\frac{1}{2m_{cv}}\sum^{m_{cv}}_{i=1}(h_θ(x_{cv}^{(i)})-y_{cv}^{(i)})^{2}</script><p>一，通常来说，一个良好的模型应该具有以下的性质：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652710284816-1653371283128.png" class width="1652710284816"> 
<ul>
<li>随着训练集数目m的增大，训练集的误差增大，测试集的误差减小</li>
</ul>
<p>二，对于高 Bias（偏差）的模型：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652710566556-1653371283128.png" class width="1652710566556"> 
<ul>
<li>随着训练集数目m的增大，训练集和测试集的误差都逐渐趋于平缓，也就意味着，不管再投入多少数据，测试集的误差都不会有明显的下降了</li>
</ul>
<p>三，对于高 Variance（方差）的模型：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652710775037-1653371283128.png" class width="1652710775037">  
<ul>
<li>不管训练集数目m再怎么增大，训练集和测试集的误差始终都有一大截“鸿沟”，不过在高方差的模型中，投入更多数据还是对训练有帮助的</li>
</ul>
<h2 id="支持向量机SVM"><a href="#支持向量机SVM" class="headerlink" title="支持向量机SVM"></a>支持向量机SVM</h2><p>与逻辑回归和神经网络相比，支持向量机（support vector machines，SVM）能提供一种更为清晰和强大的方式，来运算 <strong>复杂的非线性方程</strong></p>
<p>我们先从逻辑回归说起：</p>
<script type="math/tex; mode=display">
h_θ=\frac1{1+e^{-θ^Tx}}</script><img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652875817979.png" class width="1652875817979"> 
<ul>
<li>当真实数据 y=1 时，我们想要 hθ(x) ≈ 1，所以 z &gt; 0</li>
<li>当真实数据 y=0 时，我们想要 hθ(x) ≈ 0，所以 z &lt; 0</li>
</ul>
<p>逻辑回归的代价函数为：交叉熵</p>
<script type="math/tex; mode=display">
J(θ)=\frac1{m}\sum_{i=1}^mCost(h_θ(x^{(i)}),y^{(i)})
=-\frac1{m}[\sum_{i=1}^my^{(i)}log\,h_θ(x^{(i)})+(1-y^{(i)})log(1-h_θ(x^{(i)}))]</script><p>把单独的 Cost 函数提取出来：</p>
<script type="math/tex; mode=display">
Cost(h_θ(x^{(i)}),y^{(i)})=-y^{(i)}log\,h_θ(x^{(i)})-(1-y^{(i)})log(1-h_θ(x^{(i)}))</script><p>把 hθ 替换为 Sigmoid 函数：</p>
<script type="math/tex; mode=display">
Cost(h_θ(x^{(i)}),y)=-y\,log\,\frac1{1+e^{-θ^Tx}}-(1-y)\,log(1-\frac1{1+e^{-θ^Tx}})</script><p>针对这个函数，SVM 其实就是做了一件如下图所示的事情：</p>
<ul>
<li><p>y = 1 时，函数的图像：取点 (1,0) 然后作一条和逻辑回归曲线相似的直线，名为 Cost1(z)</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652876615894.png" class width="1652876615894"> 
</li>
<li><p>y = 0 时，函数的图像：取点 (-1,0) 然后作一条和逻辑回归曲线相似的直线，名为 Cost0(z)</p>
</li>
</ul>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652876762133.png" class width="1652876762133"> 
<p>SVM 就是把两条与逻辑回归曲线相似的<strong>直线</strong>，当做了代价函数，这样虽然牺牲了一点准确度，但是大大降低了运算的难度</p>
<p>对此，我们只需要把交叉熵简单修改一下，就可以得到 SVM 的代价函数</p>
<ul>
<li>交叉熵：</li>
</ul>
<script type="math/tex; mode=display">
J(θ)=\frac1{m}[\sum_{i=1}^my^{(i)}(-log\,h_θ(x^{(i)}))+(1-y^{(i)})(-log(1-h_θ(x^{(i)})))]
+\frac{λ}{2m}\sum_{j=1}^nθ_j^{2}</script><ul>
<li>SVM 的代价函数：（注意这里去掉了“m”，而“C”是一个常数）</li>
</ul>
<script type="math/tex; mode=display">
J(θ)=C\sum_{i=1}^my^{(i)}Cost_1(θ^{T}x^{(i)})+(1-y^{(i)})Cost_0(θ^{T}x^{(i)})
+\frac{1}{2}\sum_{j=1}^nθ_j^{2}</script><img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652878030260.png" class width="1652878030260"> 
<ul>
<li>当真实数据 y=1 时，我们想要 hθ(x) = 1，所以 z &gt; 1（而不仅仅是 z &gt; 0）</li>
<li>当真实数据 y=0 时，我们想要 hθ(x) = 0，所以 z &lt; -1（而不仅仅是 z &lt; 0）</li>
</ul>
<p>和逻辑回归不同，SVM 对于分类的判断不是“连续”的，而是有一个“间隔”，这就相当于在SVM中构建了一个安全因子（安全距离）</p>
<p>案例一：假设有一个 y=1 的训练样本，常量 C 很大</p>
<ul>
<li>因为 y=1 ，所以我们只用看 Cost1</li>
<li>算法的目的是：预测正确 =&gt; hθ(x)≈1 =&gt; J(θ)尽可能小 =&gt; θ尽可能小</li>
<li>这样，公式就可以简化为：</li>
</ul>
<script type="math/tex; mode=display">
J(θ)=C\sum_{i=1}^mCost_1(1)+\frac{1}{2}\sum_{j=1}^nθ_j^{2}=
\frac{1}{2}\sum_{j=1}^nθ_j^{2}</script><ul>
<li>最小化代价函数，其实就是使 θ 的平方和最小</li>
<li>PS：这里的 θ 其实是向量，具体的数学原理和向量内积有关</li>
</ul>
<p>案例二：为数据集画决策边界</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652882030322.png" class width="1652882030322"> 
<ul>
<li>紫线，绿线，都是不合理的决策边界</li>
<li>而 SVM 会选择黑线，因为黑线距离样本的最小距离更大一些</li>
<li>看看两条蓝线的距离，这个距离就叫做<strong>支持向量机的间距</strong></li>
</ul>
<p>因为 SVM 每次都会尽量用更大的间距去分离，所以它也被称为大间距分类器</p>
<p>案例三：为数据集画决策边界</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1652883071042.png" class width="1652883071042"> 
<ul>
<li>对于这个样本，SVM 有两种画法：黑线和紫线</li>
<li>具体采用谁为决策边界是常数C决定的：<ul>
<li>C 比较小：采用黑线</li>
<li>C 比较大：采用紫线</li>
</ul>
</li>
<li>所以我们可以通过调节C，来改变 SVM 的决策方式</li>
</ul>
<h2 id="高斯核函数"><a href="#高斯核函数" class="headerlink" title="高斯核函数"></a>高斯核函数</h2><p>核函数也被称为相似度函数，用于模拟 <strong>非线性</strong> 决策边界的特征</p>
<p>接下来就通过一个案例来理解核函数的使用场景和作用：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1653151038604.png" class width="1653151038604">  
<p>假设有一块非线性的区域，我们想拟合一个决策边界来区分正负实例，通常有两种办法：</p>
<script type="math/tex; mode=display">
θ_0+θ_1x_1+θ_2x_2+θ_3x_1x_2+θ_4x_1^2+θ_5x_2^2...\\
θ_0+θ_1f_1+θ_2f_2+θ_3f_3+θ_4f_4+θ_5f_5...</script><ul>
<li>第一种方法：通过构建高次项来拟合更复杂的决策边界</li>
<li>第二种方法：通过高斯核函数来定义这些特征值</li>
</ul>
<p>第一种方法会面临“过拟合”，“欠拟合”等一系列问题，所以第二种方法诞生了，以下图为例：</p>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1653152301671.png" class width="1653152301671"> 
<ul>
<li>我们先手动取“3”个点：L1，L2，L3，把这些点成为“标记”</li>
<li>然后用高斯核函数来定义特征：</li>
</ul>
<script type="math/tex; mode=display">
f_1=similarity(x,L^{(1)})=exp(-\frac{||x-l^{(1)}||^{2}}{2σ^{2}})\\
f_1=similarity(x,L^{(2)})=exp(-\frac{||x-l^{(2)}||^{2}}{2σ^{2}})\\
f_1=similarity(x,L^{(3)})=exp(-\frac{||x-l^{(3)}||^{2}}{2σ^{2}})</script><ul>
<li>PS：||X-Y|| 就是向量作差之后各分量的平方和的开根号</li>
</ul>
<p>现在解释一下高斯核函数：</p>
<ul>
<li>但点 (x1，x2) 靠近 L(i) 时，x ≈ L(i) ，f(i) ≈ exp(0) ≈ 1</li>
<li>但点 (x1，x2) 远离 L(i) 时，x !≈ L(i) ，f(i) ≈ exp(∞) ≈ 0</li>
</ul>
<p>正样本的附近大概率也是正样本，依照这个规律，相似度函数会根据目标和“标记”之间的距离，来判断该样本是不是正样本，现在再回看一下决策边界的基本模型：</p>
<script type="math/tex; mode=display">
θ_0+θ_1f_1+θ_2f_2+θ_3f_3+θ_4f_4+θ_5f_5...</script><p>把每一个相似度函数的结果进行整合（“标记”越多，最终结果的判断也越精确），作为特征用于 θ 的拟合，然后就可以用 SVM 对模型进行训练了，那么接下来的问题就是：如何选取 L1，L2，L3…</p>
<ul>
<li>而我们的解决办法也是简单暴力：直接把样本数据当做“标记”</li>
<li>对于每一个样本 (x,y,z…) 都可以被放入相似度函数中，生成 n 个 f (x,y,z…)</li>
</ul>
<p>高斯核函数的完整公式为：</p>
<script type="math/tex; mode=display">
K_{gaussian}(x^{(i)},x^{(j)})
=exp(-\frac{||x^{(i)}-x^{(j)}||^{2}}{2σ^{2}})
=exp(-\frac{1}{2σ^{2}}\sum_{k=1}^n(x^{(i)}_k-x^{(j)}_k)^{2})</script><p>在描绘 <strong>非线性决策边界</strong> 时，单纯的 SVM 算法起不到良好的效果，而如果用高斯核函数来定义特征的话，效果会好很多：</p>
<ul>
<li>只使用 SVM 算法：</li>
</ul>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1653205137538.png" class width="1653205137538">   
<ul>
<li>同时使用 SVM 算法和高斯核函数：</li>
</ul>
<img src="/2022/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1653205186620.png" class width="1653205186620">  

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"><i class="fa fa-tag"></i> Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/05/08/Machine-Learning-Lab2/" rel="prev" title="Machine-Learning-Lab2">
      <i class="fa fa-chevron-left"></i> Machine-Learning-Lab2
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/05/09/Kernel%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" rel="next" title="Kernel基础知识（持续更新）">
      Kernel基础知识（持续更新） <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.</span> <span class="nav-text">监督学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.</span> <span class="nav-text">无监督学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E5%9B%9E%E5%BD%92"><span class="nav-number">3.</span> <span class="nav-text">线性回归-回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92-%E5%9B%9E%E5%BD%92"><span class="nav-number">4.</span> <span class="nav-text">多项式回归-回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-number">5.</span> <span class="nav-text">均方误差-代价函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-%E6%8B%9F%E5%90%88%E5%87%BD%E6%95%B0"><span class="nav-number">6.</span> <span class="nav-text">梯度下降-拟合函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B-%E6%8B%9F%E5%90%88%E5%87%BD%E6%95%B0"><span class="nav-number">7.</span> <span class="nav-text">正规方程-拟合函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E8%A7%84%E8%8C%83%E5%8C%96-%E4%BC%98%E5%8C%96"><span class="nav-number">8.</span> <span class="nav-text">特征规范化-优化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB"><span class="nav-number">9.</span> <span class="nav-text">逻辑回归-分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%81%87%E8%AE%BE%E9%99%88%E8%BF%B0-%E5%88%86%E7%B1%BB"><span class="nav-number">10.</span> <span class="nav-text">假设陈述-分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C-%E5%88%86%E7%B1%BB"><span class="nav-number">11.</span> <span class="nav-text">决策边界-分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB-%E5%88%86%E7%B1%BB"><span class="nav-number">12.</span> <span class="nav-text">多元分类-分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-number">13.</span> <span class="nav-text">交叉熵-代价函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fminunc-%E6%8B%9F%E5%90%88%E5%87%BD%E6%95%B0"><span class="nav-number">14.</span> <span class="nav-text">fminunc-拟合函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98-%E4%BC%98%E5%8C%96"><span class="nav-number">15.</span> <span class="nav-text">过拟合问题-优化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96-%E4%BC%98%E5%8C%96-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-number">16.</span> <span class="nav-text">正则化-优化-代价函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96-%E4%BC%98%E5%8C%96-%E6%8B%9F%E5%90%88%E5%87%BD%E6%95%B0"><span class="nav-number">17.</span> <span class="nav-text">正则化-优化-拟合函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E5%8C%96-%E4%BC%98%E5%8C%96-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-number">18.</span> <span class="nav-text">向量化-优化-代价函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">19.</span> <span class="nav-text">神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-number">20.</span> <span class="nav-text">交叉熵(神经网络)-代价函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-%E4%BC%98%E5%8C%96"><span class="nav-number">21.</span> <span class="nav-text">反向传播-优化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%A3%80%E6%B5%8B-%E4%BC%98%E5%8C%96"><span class="nav-number">22.</span> <span class="nav-text">梯度检测-优化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96-%E4%BC%98%E5%8C%96"><span class="nav-number">23.</span> <span class="nav-text">随机初始化-优化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE-%E8%AF%AF%E5%B7%AE"><span class="nav-number">24.</span> <span class="nav-text">偏差方差-误差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81-%E8%AF%AF%E5%B7%AE"><span class="nav-number">25.</span> <span class="nav-text">交叉验证-误差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF-%E8%AF%AF%E5%B7%AE"><span class="nav-number">26.</span> <span class="nav-text">学习曲线-误差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM"><span class="nav-number">27.</span> <span class="nav-text">支持向量机SVM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%AB%98%E6%96%AF%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="nav-number">28.</span> <span class="nav-text">高斯核函数</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="yhellow"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">yhellow</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">171</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">111</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yhellow</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">2.3m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">34:21</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
